# -*- coding: utf-8 -*-
"""
pipeline.py â€” è¯åº“è‡ªåŠ¨åŒ–æœ€ç»ˆç‰ˆ

åŠŸèƒ½æ€»è§ˆï¼š
- wordfreq å–è¯ï¼ˆæ”¯æŒ AUTO_OFFSETï¼‰
- LLM ç”ŸæˆåŸºç¡€ JSONï¼ˆå¤šè¯æ€§ + æ¯ä¸ªè¯æ€§æœ‰å¯¹åº”ä¸­æ–‡é‡Šä¹‰ï¼›è‡³å°‘ 1 ä¸ª phrase + AB å¯¹è¯ï¼‰
- JSON Schema éªŒè¯ + è´¨é‡æ£€æŸ¥ï¼ˆçŸ­è¯­/å¯¹è¯å®½æ¾åŒ¹é…ï¼›åŠŸèƒ½è¯è±å…ï¼›è¦†ç›–ç‡æ ¡éªŒï¼‰
- äºŒæ¬¡è¡¥æ•‘ï¼ˆä¸“ç”¨ Promptï¼‰
- è‹¥ä»å¤±è´¥ï¼šè‡ªåŠ¨è¿è¡Œ auto_whitelist.py -> é‡æ–°åŠ è½½ç™½åå• -> å¯¹å¤±è´¥è¯å†è¡¥æ•‘ä¸€æ¬¡
- äºŒæ¬¡ä»å¤±è´¥ï¼šå†™å…¥ failed_permanent.jsonï¼ˆå•ä¸€æ–‡ä»¶ï¼Œè¿½åŠ å»é‡ï¼‰å¹¶å¼ºåˆ¶æ¸…ç†ä¸´æ—¶å¤±è´¥æ—¥å¿—
- æˆåŠŸæ—¶ï¼šæ¸…ç†æ‰€æœ‰å¤±è´¥æ—¥å¿—
- æ–­ç‚¹ç»­è·‘ï¼šstate.json ä¸è¿è¡Œé…ç½®ç»‘å®š
- ä¾‹å¥ï¼šTatoeba EN-ZH TSVï¼Œç®€ä½“ä¼˜å…ˆï¼›å£è¯­è¯„åˆ†é€‰æ‹©
- IPAï¼šCMUdict ç¾éŸ³
- å¹¶å‘ï¼šå¯é…ç½® WORKERSï¼ˆåˆå¹¶/å†™å…¥åŠ é”ï¼‰
"""

import os, re, json, time, shutil, glob, subprocess, threading, hashlib
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from tqdm import tqdm
from wordfreq import top_n_list
import cmudict
from jsonschema import validate
from opencc import OpenCC

# ===================== åŸºæœ¬é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ =====================
TOTAL_WORDS     = 1000         # æœ¬è½®ç›®æ ‡è¯æ•°ï¼ˆä¼˜åŒ–ï¼šå¢åŠ åˆ°1000è¯ï¼‰
BATCH_SIZE      = 8            # ä¸»æ‰¹æ¬¡å¤§å°ï¼ˆä¼˜åŒ–ï¼šé™ä½åˆ°8ä»¥æé«˜æˆåŠŸç‡ï¼‰
MIN_BATCH_SIZE  = 3            # è‡ªé€‚åº”åˆ†ç‰‡æœ€å°ï¼ˆä¼˜åŒ–ï¼šé™ä½æœ€å°åˆ†ç‰‡ï¼‰
RETRY_BATCH_SIZE= 3            # äºŒæ¬¡è¡¥æ•‘çš„å°æ‰¹é‡ï¼ˆä¼˜åŒ–ï¼šæ›´å°æ‰¹æ¬¡ç²¾ç¡®å¤„ç†ï¼‰
MAX_RETRY       = 2            # ä¸»æ‰¹å†…é‡è¯•æ¬¡æ•°
RETRY_MAX_RETRY = 2            # äºŒæ¬¡è¡¥æ•‘å†…é‡è¯•æ¬¡æ•°ï¼ˆä¼˜åŒ–ï¼šå¿«é€Ÿå¤±è´¥é¿å…æ— æ•ˆé‡è¯•ï¼‰
TEMPERATURE     = 0.1
WORKERS         = 2            # å¹¶å‘ worker æ•°ï¼ˆ1=ä¸²è¡Œï¼›2 æ›´å¿«ï¼›æ³¨æ„ API é™æµï¼‰
TIMEOUT_SEC     = 90           # è¯·æ±‚è¶…æ—¶ï¼ˆå¢åŠ åˆ°90ç§’ï¼‰

# è¯åŒºé—´ï¼ˆå›ºå®šåŒºé—´ï¼Œé¿å…é‡å¤è·å–ï¼‰
AUTO_OFFSET     = False        # True: è‡ªåŠ¨ç”¨ç°æœ‰ words.json çš„é•¿åº¦ä½œä¸º OFFSET
OFFSET          = 3000         # Resource4å¤„ç†3000-3999è¯æ±‡

# é¢„åˆ¤è·³è¿‡ï¼šå·²å­˜åœ¨çš„è¯æ˜¯å¦è·³è¿‡ä¸è¯·æ±‚ AIï¼ˆçœé’±çœæ—¶ï¼‰
SKIP_EXISTING   = True

# æ•°æ®/æ–‡ä»¶
WORDS_REPO      = "words.json"
BACKUP_DIR      = "backup"
LOG_DIR         = "logs"
STATE_FILE      = "state.json"
TATO_FILE       = os.path.join("data", "tatoeba_en_zh.tsv")
PERM_FAILED_FILE= os.path.join("..", "Resource", "failed_permanent.json")     # æ°¸ä¹…å¤±è´¥è¯ï¼ˆç»Ÿä¸€å†™å…¥Resourceç›®å½•ï¼‰
FUNCTION_WORDS_FILE = "function_words_custom.txt"  # å¤–éƒ¨è‡ªå®šä¹‰ç™½åå•

# DeepSeek
MODEL_NAME      = "glm-4.6"
DEEPSEEK_API_KEY = "sk-550080883bc649498b2f4897e5df2929"  # DeepSeek API Key

# æ—¥å¿—æ¸…ç†ç­–ç•¥
LOG_CLEAN_ON_SUCCESS = True    # æœ¬è½®æ— å¤±è´¥ -> æ¸…ç†æ‰€æœ‰å¤±è´¥æ—¥å¿—
LOG_PRUNE_RESOLVED   = True    # æœ¬è½®ä»æœ‰å¤±è´¥ -> ä¹Ÿæ¸…ç†é‚£äº›â€œå·²è¢«è¡¥é½â€çš„å†å²å¤±è´¥æ—¥å¿—
LOG_KEEP_LAST_RUNS   = 5       # run_*.log ä¿ç•™æœ€è¿‘ N ä»½

# =========== å¤šè¯æ€§ + æ¯ä¸ªè¯æ€§æœ‰å¯¹åº”ä¸­æ–‡é‡Šä¹‰ï¼šSchema ===========
SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "required": ["word", "parts_of_speech", "pos_meanings", "phrases", "sentences", "pronunciation"],
        "properties": {
            "word": {"type": "string"},
            "parts_of_speech": {
                "type": "array",
                "items": {"type": "string"},
                "minItems": 1
            },
            # pos_meanings: [{pos:"verb", cn:"ä¸­æ–‡é‡Šä¹‰"}, ...]
            "pos_meanings": {
                "type": "array",
                "items": {
                    "type": "object",
                    "required": ["pos", "cn"],
                    "properties": {
                        "pos": {"type": "string"},
                        "cn":  {"type": "string"}
                    }
                },
                "minItems": 1
            },
            "phrases": {
                "type": "array",
                "items": {
                    "type": "object",
                    "required": ["phrase", "dialog"],
                    "properties": {
                        "phrase": {"type": "string"},
                        "dialog": {
                            "type": "object",
                            "required": ["A", "B"],
                            "properties": {
                                "A": {"type": "string"},
                                "B": {"type": "string"}
                            }
                        }
                    }
                },
                "minItems": 0
            },
            "sentences": {"type": "array"},  # ç”±è„šæœ¬è¡¥é½
            "pronunciation": {
                "type": "object",
                "required": ["US"],
                "properties": {
                    "US": {
                        "type": "object",
                        "required": ["ipa", "audio"],
                        "properties": {
                            "ipa":   {"type": ["string", "null"]},
                            "audio": {"type": ["string", "null"]}
                        }
                    }
                }
            }
        }
    }
}

# ===================== è¿è¡Œ IDï¼ˆå˜é…ç½®åˆ™é‡ç½® stateï¼‰ =====================
def _load_repo_dict():
    if not os.path.exists(WORDS_REPO):
        return {}
    try:
        data = json.load(open(WORDS_REPO, "r", encoding="utf-8"))
        if not isinstance(data, list):
            return {}
        return { (it.get("word") or "").lower(): it for it in data if isinstance(it, dict) }
    except:
        return {}

def _save_repo(repo_dict: dict):
    os.makedirs(BACKUP_DIR, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    # å¤‡ä»½
    with open(os.path.join(BACKUP_DIR, f"words_{ts}.json"), "w", encoding="utf-8") as bf:
        json.dump(list(repo_dict.values()), bf, ensure_ascii=False, indent=2)
    # å†™ä¸»æ–‡ä»¶
    with open(WORDS_REPO, "w", encoding="utf-8") as f:
        json.dump(list(repo_dict.values()), f, ensure_ascii=False, indent=2)

def _count_repo(repo_dict): return len(repo_dict)

repo_lock  = threading.Lock()
state_lock = threading.Lock()

def _compute_offset(repo_dict):
    return _count_repo(repo_dict)

# run_id ç»‘å®š offset/total/batch
def _run_id(offset, total, batch):
    return f"off{offset}-tot{total}-bs{batch}"

def batch_key(words):
    # ç”¨è¯åˆ—è¡¨ç”Ÿæˆç¨³å®š keyï¼ˆå°å†™ + é€—å·æ‹¼æ¥ï¼‰
    s = ",".join([w.lower() for w in words])
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def load_state(RUN_ID):
    if not os.path.exists(STATE_FILE):
        return {"run_id": RUN_ID, "done_batches": []}
    try:
        st = json.load(open(STATE_FILE, "r", encoding="utf-8"))
        old_run_id = st.get("run_id")
        if old_run_id != RUN_ID:
            print(f"ğŸ”„ å‚æ•°å˜æ›´ï¼šæ—§RUN_ID={old_run_id} -> æ–°RUN_ID={RUN_ID}ï¼Œæ¸…ç©ºçŠ¶æ€")
            return {"run_id": RUN_ID, "done_batches": []}
        st["done_batches"] = list(set(st.get("done_batches", [])))
        return st
    except:
        return {"run_id": RUN_ID, "done_batches": []}

def save_state(state, RUN_ID):
    state["run_id"] = RUN_ID
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

# ===================== åŠŸèƒ½è¯ç™½åå•ï¼ˆå¤–éƒ¨æ–‡ä»¶ + é»‘åå•å…œåº•ï¼‰ =====================
BASE_FUNCTION_WORDS = {
    "a","an","the","some","any","each","every","either","neither","another",
    "this","that","these","those","both","all","half","many","much","few","little",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","her","its","our","their","mine","yours","hers","ours","theirs",
    "someone","anyone","everyone","no one","something","anything","everything","nothing",
    "to","of","in","on","at","for","from","by","with","about","as","after","before",
    "between","around","since","than","over","under","into","onto","through","during",
    "without","within","across","against","toward","upon","off","up","down","out",
    "and","or","but","if","so","yet","nor","though","although","because","while","unless",
    "be","am","is","are","was","were","been","being",
    "do","does","did","done","doing",
    "have","has","had","having",
    "can","could","may","might","must","shall","should","will","would",
    "not","n't","no","only","very","just",
    "i'm","you're","he's","she's","it's","we're","they're",
    "i've","you've","we've","they've",
    "i'd","you'd","he'd","she'd","we'd","they'd",
    "i'll","you'll","he'll","she'll","we'll","they'll",
    "don't","doesn't","didn't","can't","couldn't","won't","wouldn't","isn't",
    "aren't","wasn't","weren't","haven't","hasn't","hadn't","shouldn't","mustn't",
}

CONTENT_BLACKLIST = {
    "help","find","show","make","take","give","get","go","come","look","work","call",
    "use","need","try","keep","think","know","feel","tell","ask","play","run","write",
    "people","person","thing","time","way","good","new","old","high","big","small",
    "better","best","always","never","end","begin","start"
}

def load_function_words():
    words = set(BASE_FUNCTION_WORDS)
    if os.path.exists(FUNCTION_WORDS_FILE):
        for line in open(FUNCTION_WORDS_FILE, "r", encoding="utf-8"):
            w = line.strip().lower()
            if not w: continue
            if w in CONTENT_BLACKLIST:  # è¯¯åŠ çš„å®è¯ä¸é‡‡çº³
                continue
            words.add(w)
    return words

FUNCTION_WORDS = load_function_words()

# ===================== Prompt =====================
PROMPT_TEMPLATE = """
ä½ è¦ä¸ºæˆ‘æä¾›çš„è‹±è¯­å•è¯ç”Ÿæˆä¸€ä¸ªä¸¥æ ¼çš„ JSON æ•°ç»„ï¼ˆä»…è¾“å‡º JSONï¼Œæ— ä»»ä½•é¢å¤–æ–‡å­—ï¼‰ã€‚
æ¯ä¸ªå…ƒç´ å¯¹è±¡å¿…é¡»åŒ…å«ï¼š
- "word": å•è¯
- "parts_of_speech": åœ¨ {noun, verb, adj, adv, pron, prep, conj, det, aux, modal, interj, num, proper_noun, abbr, article, phrasal_verb} ä¸­é€‰æ‹©åˆé€‚çš„ä¸€ä¸ªæˆ–å¤šä¸ª
- "pos_meanings": æ•°ç»„ï¼›æ¯ä¸ªå…ƒç´ å½¢å¦‚ { "pos": "verb", "cn": "ç®€çŸ­ä¸­æ–‡é‡Šä¹‰ï¼ˆâ‰¤30å­—ï¼‰" }ã€‚
  è¦æ±‚ï¼šåˆ—å‡ºçš„ pos ä¸ parts_of_speech ä¸€ä¸€å¯¹åº”ï¼Œæ¯ä¸ª pos å¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç®€çŸ­ä¸­æ–‡é‡Šä¹‰ã€‚
- "phrases": æ•°ç»„ï¼š
- éåŠŸèƒ½è¯ï¼ˆå/åŠ¨/å½¢/å‰¯ç­‰ï¼‰ï¼šè‡³å°‘ 1 ä¸ªå¯¹è±¡ï¼ŒçŸ­è¯­é¡»ä¸ºè‡ªç„¶æ­é…ï¼Œä¸èƒ½ç­‰äºå•è¯æœ¬ä½“
- åŠŸèƒ½è¯ï¼ˆå† è¯/ä»£è¯/ä»‹è¯/è¿è¯/åŠ©åŠ¨/æƒ…æ€/æ„Ÿå¹/æ•°è¯ç­‰ï¼‰ï¼šå…è®¸ä¸ºç©º []
  [
    {
      "phrase": "åŒ…å«è¯¥å•è¯æœ¬ä½“çš„å¸¸è§è¯ç»„ï¼ˆå¦‚ help out / find out / show up / deal withï¼‰",
      "dialog": { "A": "ä¸€å¥è‡ªç„¶å£è¯­ï¼Œâ‰¤12è¯", "B": "è‡ªç„¶å›å¤ï¼Œâ‰¤12è¯" }
    }
  ]
- "sentences": å›ºå®šä¸º []
- "pronunciation": å›ºå®šä¸º { "US": { "ipa": null, "audio": null } }

ä¸¥æ ¼è¦æ±‚ï¼š
1) åªè¿”å› JSON æ•°ç»„ï¼›é•¿åº¦å¿…é¡»ä¸æˆ‘æä¾›çš„å•è¯æ•°ä¸€è‡´ã€‚
2) phrases.dialog çš„ A æˆ– B å¿…é¡»é€å­—åŒ…å« phrases.phraseï¼›ä¸å¾—ç”¨å•è¯æœ¬ä½“å……å½“çŸ­è¯­ã€‚
3) è¾“å‡ºç²¾ç®€ï¼šæ¯ä¸ªä¸­æ–‡é‡Šä¹‰ â‰¤ 30 å­—ï¼›å¯¹è¯å£è¯­è‡ªç„¶ï¼Œé¿å…åŒä¹‰æ›¿æ¢å¯¼è‡´çŸ­è¯­ç¼ºå¤±ã€‚
4) word å­—æ®µå¿…é¡»ä¸è¾“å…¥å•è¯å®Œå…¨ä¸€è‡´ï¼ˆåŒ…å«å¤§å°å†™ä¸æ’‡å·ï¼‰ï¼Œä¸å¾—æ”¹å†™æˆ–è§„èŒƒåŒ–ã€‚
å•è¯åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼š
<<WORDS>>
""".strip()

RESCUE_PROMPT_TEMPLATE = """
ä¸ºæˆ‘æä¾›çš„è‹±è¯­å•è¯ç”Ÿæˆä¸¥æ ¼æ ¼å¼çš„ JSON æ•°ç»„ï¼ˆä»…è¾“å‡º JSONï¼‰ã€‚
æ¯ä¸ªå¯¹è±¡å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼š
{
  "word": "å•è¯",
  "parts_of_speech": ["noun", "verb"],
  "pos_meanings": [
    {"pos": "noun", "cn": "åè¯é‡Šä¹‰â‰¤30å­—"},
    {"pos": "verb", "cn": "åŠ¨è¯é‡Šä¹‰â‰¤30å­—"}
  ],
  "phrases": [
    {
      "phrase": "çœŸå®çš„çŸ­è¯­è¯ç»„ï¼ˆå¦‚make up/look at/find outï¼Œç»å¯¹ä¸èƒ½æ˜¯å•è¯æœ¬ä½“ï¼‰",
      "dialog": {
        "A": "è‡ªç„¶å¯¹è¯â‰¤12è¯åŒ…å«çŸ­è¯­",
        "B": "å›å¤â‰¤12è¯"
      }
    }
  ],
  "sentences": [],
  "pronunciation": {"US": {"ipa": null, "audio": null}}
}

é‡è¦è§„åˆ™ï¼š
1. pos_meaningså¿…é¡»æ˜¯å¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«poså’Œcnä¸¤ä¸ªå­—æ®µï¼
2. phraseç»å¯¹ä¸èƒ½æ˜¯å•è¯æœ¬ä½“ï¼å¿…é¡»æ˜¯çœŸå®çš„çŸ­è¯­è¯ç»„ï¼
3. å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„çŸ­è¯­ï¼Œphraseså¯ä»¥ä¸ºç©ºæ•°ç»„ []
åªè¾“å‡ºJSONæ•°ç»„ï¼Œé•¿åº¦ä¸è¾“å…¥å•è¯æ•°ä¸€è‡´ã€‚

å•è¯åˆ—è¡¨ï¼š
<<WORDS>>
""".strip()

# åŠŸèƒ½è¯ä¸“ç”¨ç®€åŒ–æ¨¡æ¿ï¼ˆç»™åŠŸèƒ½è¯ä¸€ä¸ªæç®€ä½†åˆè§„çš„çŸ­è¯­ä¸å¯¹è¯ï¼‰
FUNCTION_WORD_PROMPT_TEMPLATE = """
ä¸ºæˆ‘æä¾›çš„åŠŸèƒ½è¯ç”Ÿæˆç®€åŒ–çš„ JSON æ•°ç»„ï¼ˆä»…è¾“å‡º JSONï¼‰ã€‚
åŠŸèƒ½è¯ï¼ˆå† è¯/ä»£è¯/ä»‹è¯/è¿è¯/åŠ©åŠ¨è¯/æƒ…æ€åŠ¨è¯ç­‰ï¼‰è¦æ±‚ï¼š
- "word": å•è¯
- "parts_of_speech": ä» {det, pron, prep, conj, aux, modal, interj, num} ä¸­é€‰æ‹©åˆé€‚çš„ä¸€ä¸ªæˆ–å¤šä¸ª
- "pos_meanings": æ•°ç»„ï¼›æ¯ä¸ªå…ƒç´ å½¢å¦‚ { "pos": "prep", "cn": "ç®€çŸ­ä¸­æ–‡é‡Šä¹‰ï¼ˆâ‰¤20å­—ï¼‰" }
- "phrases": åŠŸèƒ½è¯å¾ˆéš¾æ„æˆç‹¬ç«‹çŸ­è¯­ï¼Œé€šå¸¸ä¸ºç©ºæ•°ç»„ []
- "sentences": å›ºå®šä¸º []
- "pronunciation": å›ºå®šä¸º { "US": { "ipa": null, "audio": null } }

é‡è¦è§„åˆ™ï¼š
1. pos_meaningså¿…é¡»æ˜¯å¯¹è±¡æ•°ç»„æ ¼å¼ï¼
2. ç»å¯¹ä¸èƒ½ç”¨å•è¯æœ¬ä½“ä½œä¸ºphraseï¼
3. åŠŸèƒ½è¯phrasesé€šå¸¸ä¸ºç©º []

åªè¿”å› JSON æ•°ç»„ï¼Œé•¿åº¦ä¸è¾“å…¥å•è¯æ•°ä¸€è‡´ã€‚

å•è¯åˆ—è¡¨ï¼š
<<WORDS>>
""".strip()

def build_prompt(batch_words):         
    # æ™ºèƒ½é€‰æ‹©æ¨¡æ¿ï¼šå¦‚æœæ‰¹æ¬¡ä¸­åŠŸèƒ½è¯å å¤šæ•°ï¼Œä½¿ç”¨åŠŸèƒ½è¯æ¨¡æ¿
    function_word_count = sum(1 for w in batch_words if w.lower() in FUNCTION_WORDS)
    if function_word_count >= len(batch_words) * 0.5:  # 50%ä»¥ä¸Šæ˜¯åŠŸèƒ½è¯
        return FUNCTION_WORD_PROMPT_TEMPLATE.replace("<<WORDS>>", ", ".join(batch_words))
    else:
        return PROMPT_TEMPLATE.replace("<<WORDS>>", ", ".join(batch_words))

def build_rescue_prompt(batch_words):  return RESCUE_PROMPT_TEMPLATE.replace("<<WORDS>>", ", ".join(batch_words))

# ===================== DeepSeek è°ƒç”¨ =====================
def call_glm(prompt: str):
    if not DEEPSEEK_API_KEY:
        raise RuntimeError("DEEPSEEK_API_KEY æœªè®¾ç½®")
    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": TEMPERATURE,
        "stream": False
    }
    print(f"â†’ è°ƒç”¨AIï¼š{len(prompt)} chars | æ¨¡å‹={MODEL_NAME}", flush=True)
    resp = requests.post(url, headers=headers, json=payload, timeout=TIMEOUT_SEC)
    print("AI returned, parsing...", flush=True)
    resp.raise_for_status()
    data = resp.json()
    content = data["choices"][0]["message"]["content"]
    return content

def extract_json_block(raw: str):
    # å°½é‡æå– JSONï¼›è‹¥å·²æ˜¯arrayæˆ–objectï¼Œç›´æ¥è¿”å›
    s = raw.strip()
    if s.startswith("{") or s.startswith("["):
        return s
    # ç®€å•æ¸…æ´—
    l = s.find("["); r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        return s[l:r+1]
    raise ValueError("æœªæ‰¾åˆ° JSON æ•°ç»„")

# ===================== çŸ­è¯­åŒ¹é…ï¼šä¸¥æ ¼ + å®½æ¾ï¼ˆå…è®¸æ’è¯ï¼‰ =====================
_WORD_RE = re.compile(r"[A-Za-z]+(?:'[A-Za-z]+)?")

def _tokenize_words(text: str):
    return _WORD_RE.findall(text.lower())

def phrase_in_text_exact(phrase: str, text: str) -> bool:
    p = re.escape(phrase).replace("\\ ", r"\s+")
    pattern = rf"(^|[\s\"'â€œâ€â€˜â€™(),.;:!?-]){p}($|[\s\"'â€œâ€â€˜â€™(),.;:!?-])"
    return re.search(pattern, text, flags=re.IGNORECASE) is not None

def phrase_in_text_loose(phrase: str, text: str, max_gap: int = 2) -> bool:
    ph = phrase.strip().lower()
    if not ph: return False
    words = ph.split()
    if len(words) == 1:
        return phrase_in_text_exact(phrase, text)
    tw = _tokenize_words(text)
    n, m = len(tw), len(words)
    if n == 0: return False
    first = words[0]
    for i in range(n):
        if tw[i] != first: continue
        pos = i; ok = True
        for j in range(1, m):
            target = words[j]; found = False
            upper = min(n, pos + 1 + max_gap + 1)
            for k in range(pos + 1, upper):
                if tw[k] == target:
                    pos = k; found = True; break
            if not found: ok = False; break
        if ok: return True
    return False

def phrase_in_text(phrase: str, text: str) -> bool:
    return phrase_in_text_exact(phrase, text) or phrase_in_text_loose(phrase, text, max_gap=2)

# ===================== è´¨é‡æ£€æŸ¥ =====================
# å…è®¸çš„æ ‡å‡† POS æ ‡ç­¾
ALLOWED_POS = {
    "noun","verb","adj","adv","pron","prep","conj","det",
    "aux","modal","interj","num","proper_noun","abbr","article","phrasal_verb"
}

# å¸¸è§åˆ«åæ˜ å°„åˆ°æ ‡å‡†æ ‡ç­¾
POS_MAP = {
    "n":"noun","v":"verb","a":"adj","adjective":"adj","adverb":"adv",
    "pronoun":"pron","preposition":"prep","conjunction":"conj","determiner":"det","article":"det","art":"det",
    "auxiliary":"aux","aux":"aux","modal verb":"modal","modal":"modal",
    "interjection":"interj","number":"num","numeral":"num",
    "proper noun":"proper_noun","proper-noun":"proper_noun",
    "phrasal verb":"phrasal_verb","phrasal-verb":"phrasal_verb"
}

def _canon_pos(p: str):
    if not p: return None
    p = (p or "").strip().lower()
    # å…ˆæŒ‰å¸¸è§åˆ†éš”åˆ‡å¼€ï¼Œé€ä¸ªæ˜ å°„ï¼Œå–ç¬¬ä¸€ä¸ªå¯è¯†åˆ«çš„
    parts = re.split(r"[\/,\s\-]+", p)
    for part in parts:
        cand = POS_MAP.get(part, part)
        if cand in ALLOWED_POS:
            return cand
    return None

def _normalize_pos_list(pos_list):
    out = []
    for p in pos_list:
        cp = _canon_pos(p)
        if cp and cp in ALLOWED_POS and cp not in out:
            out.append(cp)
    return out

def _normalize_pos_meanings(pos_meanings):
    out = []
    for it in pos_meanings:
        if not isinstance(it, dict): continue
        pos = _canon_pos(it.get("pos"))
        cn  = (it.get("cn") or "").strip()
        if pos and pos in ALLOWED_POS and cn:
            out.append({"pos": pos, "cn": cn})
    return out

def check_pos_alignment(item):
    """ç¡®ä¿ parts_of_speech ä¸ pos_meanings ä¸€ä¸€å¯¹åº”"""
    pos_list = _normalize_pos_list(item.get("parts_of_speech", []))
    pm_list  = _normalize_pos_meanings(item.get("pos_meanings", []))
    if not pos_list or not pm_list:
        return False, "pos-empty"
    pos_set = set(pos_list)
    pm_pos  = {x["pos"] for x in pm_list}
    if pos_set != pm_pos:
        return False, f"pos-mismatch: pos={sorted(pos_set)} pm={sorted(pm_pos)}"
    return True, ""

def quality_checks(items):
    bad = []
    for it in items:
        w = (it.get("word") or "").lower().strip() or "?"
        
        # === åŠŸèƒ½è¯è±å…ï¼šåªéœ€åŸºæœ¬posæ£€æŸ¥ï¼Œæ— éœ€phrase/dialog ===
        if w in FUNCTION_WORDS:
            # å¯¹åŠŸèƒ½è¯ä½¿ç”¨å®½æ¾çš„posæ£€æŸ¥
            pos_list = _normalize_pos_list(it.get("parts_of_speech", []))
            pm_list  = _normalize_pos_meanings(it.get("pos_meanings", []))
            # åŠŸèƒ½è¯åªè¦æœ‰ä»»ä½•posä¿¡æ¯å°±é€šè¿‡
            if not pos_list and not pm_list:
                bad.append((w, "function-word-pos-empty")); continue
            # åŠŸèƒ½è¯ä¸è¦æ±‚posä¸pos_meaningsä¸¥æ ¼å¯¹é½ï¼Œåªè¦ä¸ä¸ºç©ºå³å¯
            continue
        
        # 1) éåŠŸèƒ½è¯çš„ä¸¥æ ¼poså¯¹é½æ£€æŸ¥
        ok, reason = check_pos_alignment(it)
        if not ok:
            bad.append((w, reason)); continue

        phr = it.get("phrases", [])

        # éåŠŸèƒ½è¯å…è®¸ç©ºphrasesï¼ˆå½“æ‰¾ä¸åˆ°åˆé€‚çœŸå®çŸ­è¯­æ—¶ï¼‰
        # å¦‚æœæœ‰phrasesï¼Œåˆ™å¿…é¡»æ˜¯çœŸå®çŸ­è¯­ï¼Œä¸èƒ½æ˜¯å•è¯æœ¬ä½“

        # 2) phrase å¿…é¡»åœ¨ A/B å‡ºç°ï¼ˆå®½æ¾ï¼‰ï¼›æˆ–è‡³å°‘ word æœ¬ä½“å‡ºç°
        ok_all = True
        for p in phr:
            phrase = (p.get("phrase") or "").strip()
            d = p.get("dialog", {}) or {}
            A = (d.get("A") or "").strip()
            B = (d.get("B") or "").strip()
            if not phrase or not A or not B:
                ok_all = False; break
                # ä¸èƒ½ç”¨å•è¯æœ¬ä½“å……å½“çŸ­è¯­
            if phrase.strip().lower() == w:
                ok_all = False; break
            # åªè®¤çŸ­è¯­å‘½ä¸­
            hit_phrase = phrase_in_text(phrase, A) or phrase_in_text(phrase, B)
            if not hit_phrase:
                ok_all = False; break
            if len(A.split()) > 20 or len(B.split()) > 20:
                ok_all = False; break
        if not ok_all:
            bad.append((w, "dialog-loose-mismatch"))
    return bad

def check_coverage(expected_words, items):
    exp = [x.lower() for x in expected_words]
    got = [ (it.get("word") or "").lower() for it in items ]
    missing = sorted(set(exp) - set(got))
    extras  = sorted(set(got) - set(exp))
    dupes   = sorted([w for w in got if got.count(w) > 1])
    return (len(missing)==0 and len(extras)==0), {"missing":missing,"extras":extras,"dupes":dupes}

# ===================== ä¾‹å¥ï¼ˆTatoebaï¼‰& IPA =====================
cc = OpenCC('t2s')  # è½¬ç®€ä½“
def _score_cn_colloquial(s):
    s0 = cc.convert(s)
    score = 0
    if any(x in s0 for x in "å§å•Šå˜›å‘¢å‘€å“ˆå•¦å’¯å‘€å‘—æŒºè›®å˜›å§å‘€å‘¢"): score += 2
    if any(x in s0 for x in "ä½ æˆ‘ä»–å¥¹å’±"): score += 1
    if any(x in s0 for x in "ä¹‹è€…è‹¥æ–¼çŸ£ç„‰"): score -= 2
    if "ï¼" in s0 or "!" in s0: score += 1  # å£è¯­æ„Ÿ
    L = len(s0)
    if L < 4 or L > 28: score -= 1
    return score, s0

def load_tatoeba(path=TATO_FILE, limit_per_word=2):
    """è¯»å– TSV: EN \t ZH ... ; è¿”å› {en: [zh1, zh2...]}"""
    if not os.path.exists(path):
        return {}
    out = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) < 4: 
                # å…¼å®¹ 2 åˆ— EN\tZH
                if len(parts) == 2:
                    en, zh = parts
                else:
                    continue
            else:
                _, en, _, zh = parts[0], parts[1], parts[2], parts[3]
            en_key = en.strip()
            zh_sc  = cc.convert(zh.strip())
            out.setdefault(en_key, []).append(zh_sc)
    # ä¸ºæ¯ä¸ª EN é€‰å‰ limit_per_word ä¸ªå£è¯­æ„Ÿæ›´å¼ºçš„
    sel = {}
    for en, arr in out.items():
        arr2 = sorted(arr, key=lambda s: _score_cn_colloquial(s), reverse=True)
        sel[en] = arr2[:limit_per_word]
    return sel

TATO = load_tatoeba()

def enrich_with_ipa_and_sentences(items):
    # ARPABETåˆ°IPAè½¬æ¢æ˜ å°„è¡¨
    arpabet_to_ipa = {
        'AA': 'É‘',   'AE': 'Ã¦',   'AH': 'ÊŒ',   'AO': 'É”',   'AW': 'aÊŠ',  'AY': 'aÉª',
        'B':  'b',   'CH': 'tÊƒ',  'D':  'd',   'DH': 'Ã°',   'EH': 'É›',   'ER': 'É',
        'EY': 'eÉª',  'F':  'f',   'G':  'g',   'HH': 'h',   'IH': 'Éª',   'IY': 'iË',
        'JH': 'dÊ’',  'K':  'k',   'L':  'l',   'M':  'm',   'N':  'n',   'NG': 'Å‹',
        'OW': 'oÊŠ',  'OY': 'É”Éª',  'P':  'p',   'R':  'r',   'S':  's',   'SH': 'Êƒ',
        'T':  't',   'TH': 'Î¸',   'UH': 'ÊŠ',   'UW': 'uË',  'V':  'v',   'W':  'w',
        'Y':  'j',   'Z':  'z',   'ZH': 'Ê’'
    }
    
    d = cmudict.dict()
    for it in items:
        w = (it.get("word") or "").lower()
        phones = d.get(w)
        ipa = None
        if phones:
            # è½¬æ¢ARPABETåˆ°IPA
            seq = phones[0]  # å–ç¬¬ä¸€ä¸ªå‘éŸ³
            ipa_symbols = []
            primary_stress_pos = -1
            secondary_stress_pos = -1
            
            # æ‰¾åˆ°é‡éŸ³ä½ç½®
            for i, phone in enumerate(seq):
                if '1' in phone:  # ä¸»é‡éŸ³
                    primary_stress_pos = i
                elif '2' in phone:  # æ¬¡é‡éŸ³
                    secondary_stress_pos = i
            
            # è½¬æ¢éŸ³ç´ å¹¶æ·»åŠ é‡éŸ³æ ‡è®°
            for i, phone in enumerate(seq):
                clean_phone = re.sub(r'\d', '', phone)  # å»æ‰æ•°å­—
                if clean_phone in arpabet_to_ipa:
                    # æ·»åŠ é‡éŸ³ç¬¦å·
                    if i == primary_stress_pos:
                        ipa_symbols.append('Ëˆ' + arpabet_to_ipa[clean_phone])
                    elif i == secondary_stress_pos:
                        ipa_symbols.append('ËŒ' + arpabet_to_ipa[clean_phone])
                    else:
                        ipa_symbols.append(arpabet_to_ipa[clean_phone])
                else:
                    # æœªçŸ¥éŸ³ç´ ï¼Œä¿æŒåŸæ ·ä½†å»æ‰æ•°å­—
                    if i == primary_stress_pos:
                        ipa_symbols.append('Ëˆ' + clean_phone)
                    elif i == secondary_stress_pos:
                        ipa_symbols.append('ËŒ' + clean_phone)
                    else:
                        ipa_symbols.append(clean_phone)
            
            ipa = "/" + "".join(ipa_symbols) + "/"
        
        it.setdefault("pronunciation", {}).setdefault("US", {})
        it["pronunciation"]["US"]["ipa"] = ipa
        it["pronunciation"]["US"]["audio"] = None

        # Tatoeba ä¾‹å¥ï¼šç®€å•ç”¨è¯¥è¯çš„è‹±æ–‡åŒ¹é…
        sents = []
        for en, zhs in TATO.items():
            if re.search(rf"(^|[^A-Za-z]){re.escape(w)}([^A-Za-z]|$)", en, flags=re.I):
                for zh in zhs:
                    sents.append({"en": en, "zh": zh})
                if len(sents) >= 2:
                    break
        it["sentences"] = sents

# ===================== å–è¯ & æ‰¹æ¬¡ =====================
def get_word_slice(offset, total, repo_dict):
    if not SKIP_EXISTING:
        # å¦‚æœä¸è·³è¿‡å·²å­˜åœ¨çš„ï¼Œä¿æŒåŸé€»è¾‘
        words_all = top_n_list("en", offset + total + 100)
        return words_all[offset : offset + total]
    
    # æ™ºèƒ½å–è¯ï¼šä¿è¯å–åˆ° total ä¸ªæ–°è¯
    new_words = []
    current_offset = offset
    batch_size = total * 3  # åˆå§‹æ‰¹é‡ï¼Œé¢„ä¼°éœ€è¦3å€è¯é‡
    
    while len(new_words) < total:
        # å–æ›´å¤šè¯é˜²æ­¢ä¸å¤Ÿ
        words_all = top_n_list("en", current_offset + batch_size)
        
        # ä»å½“å‰ä½ç½®å¼€å§‹æ‰¾æ–°è¯
        found_new = 0
        for i in range(current_offset, min(len(words_all), current_offset + batch_size)):
            word = words_all[i]
            if word.lower() not in repo_dict:
                new_words.append(word)
                found_new += 1
                if len(new_words) >= total:
                    break
        
        current_offset += batch_size
        
        # å¦‚æœè¿™æ‰¹æ²¡æ‰¾åˆ°æ–°è¯ä¸”å·²ç»è¶…å‡ºè¯å…¸èŒƒå›´ï¼Œåœæ­¢
        if found_new == 0 and current_offset >= len(words_all):
            print(f"âš ï¸  è­¦å‘Šï¼šwordfreqè¯å…¸ä¸­åªæ‰¾åˆ° {len(new_words)} ä¸ªæ–°è¯ï¼Œå°‘äºç›®æ ‡ {total} ä¸ª")
            break
    
    return new_words[:total]  # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡

def split_batches(words, batch_size):
    return [words[i:i+batch_size] for i in range(0, len(words), batch_size)]

# ===================== æ™ºèƒ½é‡è¯•æœºåˆ¶ =====================
def create_smart_retry_prompt(batch_words, error_reason):
    """æ ¹æ®å¤±è´¥åŸå› åˆ›å»ºæ™ºèƒ½é‡è¯•Prompt"""
    
    # æ‰€æœ‰åˆ†æ”¯ç»Ÿä¸€ä½¿ç”¨è¿™ä¸ªåŸºç¡€æ¨¡æ¿ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´
    base_template = f"""
ä¸ºä»¥ä¸‹è‹±è¯­å•è¯ç”Ÿæˆä¸¥æ ¼æ ¼å¼çš„ JSON æ•°ç»„ï¼ˆä»…è¾“å‡º JSONï¼‰ï¼š
[
  {{
    "word": "å•è¯",
    "parts_of_speech": ["noun"],
    "pos_meanings": [
      {{"pos": "noun", "cn": "ä¸­æ–‡é‡Šä¹‰"}}
    ],
    "phrases": [],
    "sentences": [],
    "pronunciation": {{"US": {{"ipa": null, "audio": null}}}}
  }}
]

å…³é”®è§„åˆ™ï¼š
1. pos_meaningså¿…é¡»æ˜¯å¯¹è±¡æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«poså’Œcnå­—æ®µï¼
2. ç»å¯¹ä¸èƒ½ç”¨å•è¯æœ¬ä½“ä½œä¸ºphraseï¼
3. å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„çœŸå®çŸ­è¯­ï¼Œphrasesä¸ºç©ºæ•°ç»„ []
4. è¾“å‡ºæ•°ç»„é•¿åº¦å¿…é¡»ç­‰äºè¾“å…¥è¯æ•°

å•è¯ï¼š{', '.join(batch_words)}
"""
    
    return base_template.strip()

def smart_retry_batch(repo_dict, batch_words, error_reason, max_smart_retry=2):
    """æ™ºèƒ½é‡è¯•ï¼šé’ˆå¯¹ç‰¹å®šå¤±è´¥ç±»å‹çš„ä¸“é—¨å¤„ç†"""
    for smart_attempt in range(max_smart_retry):
        try:
            smart_prompt = create_smart_retry_prompt(batch_words, error_reason)
            raw = call_glm(smart_prompt)
            data = json.loads(extract_json_block(raw))
            validate(instance=data, schema=SCHEMA)
            
            # ä½¿ç”¨å®½æ¾è´¨é‡æ£€æŸ¥
            bad = quality_checks(data)
            if bad:
                continue  # ä»æœ‰é—®é¢˜ï¼Œç»§ç»­æ™ºèƒ½é‡è¯•
                
            ok, diff = check_coverage(batch_words, [it for it in data])
            if not ok:
                continue  # è¦†ç›–ç‡é—®é¢˜ï¼Œç»§ç»­é‡è¯•
            
            # æˆåŠŸï¼šåˆå¹¶æ•°æ®
            with repo_lock:
                added = updated = 0
                for it in data:
                    w = (it.get("word") or "").lower()
                    if w in repo_dict: updated += 1
                    else: added += 1
                    repo_dict[w] = it
                _save_repo(repo_dict)
            return True, added, updated, len(batch_words), ""
            
        except Exception as e:
            continue  # ç»§ç»­å°è¯•
    
    return False, 0, 0, len(batch_words), f"smart-retry-failed after {max_smart_retry} attempts"

# ===================== æ‰§è¡Œä¸€æ‰¹ =====================
def run_batch(repo_dict, batch_words, max_retry, prompt_builder, prompt_override=None):
    last_err = ""
    validation_errors = 0  # è¿ç»­ValidationErrorè®¡æ•°
    
    for attempt in range(max_retry + 1):
        try:
            prompt = prompt_override or prompt_builder(batch_words)
            raw = call_glm(prompt)
            data = json.loads(extract_json_block(raw))
            validate(instance=data, schema=SCHEMA)

            # ç»Ÿä¸€åŒ– pos/pos_meanings
            for it in data:
                it["parts_of_speech"] = _normalize_pos_list(it.get("parts_of_speech", []))
                it["pos_meanings"]    = _normalize_pos_meanings(it.get("pos_meanings", []))

            bad = quality_checks(data)
            if bad:
                raise ValueError(f"è´¨é‡æ£€æŸ¥ä¸é€šè¿‡ {len(bad)} æ¡ï¼ˆç¤ºä¾‹ï¼‰: {bad[:5]}")
            ok, diff = check_coverage(batch_words, [it for it in data])
            if not ok:
                raise ValueError(f"è¦†ç›–ç‡å¤±è´¥: ç¼º {len(diff['missing'])}, å¤š {len(diff['extras'])}, é‡å¤ {len(diff['dupes'])}")

            # åˆå¹¶ + è½ç›˜ï¼ˆåŠ é”ï¼‰
            with repo_lock:
                added = updated = 0
                for it in data:
                    w = (it.get("word") or "").lower()
                    if w in repo_dict: updated += 1
                    else: added += 1
                    repo_dict[w] = it
                _save_repo(repo_dict)
            return True, added, updated, len(batch_words), ""
        except Exception as e:
            last_err = repr(e)
            print("DEBUG_FAIL_SAMPLE:", last_err, flush=True)
            
            # ValidationErrorå¿«é€Ÿå¤±è´¥æœºåˆ¶
            if "ValidationError" in last_err:
                validation_errors += 1
                if validation_errors >= 2:  # è¿ç»­2æ¬¡ValidationErrorå°±å¿«é€Ÿå¤±è´¥
                    print(f"âŒ è¿ç»­{validation_errors}æ¬¡ValidationErrorï¼Œå¿«é€Ÿå¤±è´¥é¿å…æ— æ•ˆé‡è¯•")
                    break
            else:
                validation_errors = 0  # é‡ç½®è®¡æ•°å™¨
            
            time.sleep(1.0)
    
    # å¸¸è§„é‡è¯•å¤±è´¥åï¼Œå°è¯•æ™ºèƒ½é‡è¯•
    print(f"å¯åŠ¨æ™ºèƒ½é‡è¯•ï¼Œé”™è¯¯ï¼š{last_err}")
    smart_ok, smart_add, smart_upd, smart_used, smart_reason = smart_retry_batch(
        repo_dict, batch_words, last_err or "unknown-error"
    )
    if smart_ok:
        print(f"âœ… æ™ºèƒ½é‡è¯•æˆåŠŸ | æ–°å¢ {smart_add} | æ›´æ–° {smart_upd}")
        return True, smart_add, smart_upd, smart_used, ""
    
    return False, 0, 0, len(batch_words), f"failed-after-smart-retry: {smart_reason}"

def run_batch_adaptive(repo_dict, words, max_retry, prompt_builder):
    """
    é€æ­¥å¤„ç†æ•´æ‰¹ wordsï¼š
    - å…ˆå°è¯•å¤§å—ï¼Œå¤±è´¥å°±å‡åŠ
    - ä¸€æ—¦æŸä¸ªå­å—æˆåŠŸï¼Œå‘å‰æ¨è¿› start æŒ‡é’ˆï¼Œç»§ç»­å¤„ç†å‰©ä½™å°¾éƒ¨
    - è¿”å›çš„ used ä¸€å®šç­‰äº len(words)ï¼ˆä¿è¯æ•´æ‰¹éƒ½è¢«å°è¯•è¿‡ï¼‰
    """
    n = len(words)
    start = 0
    added_total = 0
    updated_total = 0
    last_reason = ""
    while start < n:
        # å½“å‰å‰©ä½™åŒºé—´
        remain = n - start
        size = min(remain, max(MIN_BATCH_SIZE, remain))  # ä»å½“å‰å‰©ä½™çš„å…¨é‡å¼€å§‹è¯•
        ok_sub = False
        # è‡ªé€‚åº”ç¼©å°å—
        while size >= MIN_BATCH_SIZE:
            ok, add, upd, _, reason = run_batch(repo_dict, words[start:start+size], max_retry, prompt_builder)
            if ok:
                added_total += add
                updated_total += upd
                start += size
                ok_sub = True
                break
            else:
                last_reason = reason
                size //= 2
        if not ok_sub:
            # å½“å‰å‰©ä½™è¿™å—åœ¨æœ€å°åˆ†ç‰‡ä»å¤±è´¥ï¼Œæ”¾å¼ƒæœ¬æ‰¹å‰©ä½™
            return False, added_total, updated_total, n, "adaptive-failed"
    return True, added_total, updated_total, n, ""

# ===================== æ—¥å¿—å·¥å…· =====================
def prune_run_logs(keep_last=LOG_KEEP_LAST_RUNS):
    if not os.path.isdir(LOG_DIR): return
    runs = sorted(glob.glob(os.path.join(LOG_DIR, "run_*.log")))
    if len(runs) > keep_last:
        for p in runs[:-keep_last]:
            try: os.remove(p)
            except: pass

def _read_failed_words_from_txt(path):
    try:
        return {w.strip().lower() for w in open(path, "r", encoding="utf-8") if w.strip()}
    except:
        return set()

def _read_failed_words_from_json(path):
    try:
        data = json.load(open(path, "r", encoding="utf-8"))
        words = set()
        if isinstance(data, list):
            for b in data:
                words.update([w.lower() for w in b.get("words", [])])
        elif isinstance(data, dict):
            for b in data.get("failed", []):
                words.update([w.lower() for w in b.get("words", [])])
        return words
    except:
        return set()

def cleanup_logs(repo_dict, clean_all_if_success=False, prune_resolved=True):
    deleted = []
    if not os.path.isdir(LOG_DIR):
        return deleted
    if clean_all_if_success:
        for pat in ("failed_words_*.txt", "failed_batches_*.json"):
            for p in glob.glob(os.path.join(LOG_DIR, pat)):
                try: os.remove(p); deleted.append(os.path.basename(p))
                except: pass
        return deleted
    if prune_resolved:
        present = set(repo_dict.keys())
        for p in glob.glob(os.path.join(LOG_DIR, "failed_words_*.txt")):
            words = _read_failed_words_from_txt(p)
            if words and words.issubset(present):
                try: os.remove(p); deleted.append(os.path.basename(p))
                except: pass
        for p in glob.glob(os.path.join(LOG_DIR, "failed_batches_*.json")):
            words = _read_failed_words_from_json(p)
            if words and words.issubset(present):
                try: os.remove(p); deleted.append(os.path.basename(p))
                except: pass
    return deleted

# ===================== ä¸»æµç¨‹ =====================
def main():
    os.makedirs(LOG_DIR, exist_ok=True)

    # è¯»å–ä»“åº“
    repo_dict = _load_repo_dict()
    initial_total = _count_repo(repo_dict)  # è®°å½•è¿è¡Œå‰è¯åº“å¤§å°

    # è‡ªåŠ¨ OFFSET
    real_offset = _compute_offset(repo_dict) if AUTO_OFFSET else OFFSET
    RUN_ID = _run_id(real_offset, TOTAL_WORDS, BATCH_SIZE)
    state = load_state(RUN_ID)
    done_batches = set(state.get("done_batches", []))  # ç°åœ¨é‡Œé¢æ”¾çš„æ˜¯ key å­—ç¬¦ä¸²

    # ç½‘ç»œè‡ªæ£€ï¼ˆä¸å½±å“æµç¨‹ï¼‰
    try:
        r = requests.get("https://api.deepseek.com/v1/models",
                         headers={"Authorization": f"Bearer {DEEPSEEK_API_KEY}"},
                         timeout=10)
        print(f"ç½‘ç»œè‡ªæ£€ï¼š{r.status_code}ï¼ˆ200/401/403è§†ä¸ºè¿é€šï¼‰")
    except Exception as e:
        print(f"ç½‘ç»œè‡ªæ£€å¤±è´¥ï¼š{repr(e)}")

    # å–è¯ & æ‰¹æ¬¡
    todo_words = get_word_slice(real_offset, TOTAL_WORDS, repo_dict)
    batches = split_batches(todo_words, BATCH_SIZE)
    total_batches = len(batches)
    print(f"è®¡åˆ’å¤„ç†ï¼šTOTAL_WORDS={TOTAL_WORDS}, BATCH_SIZE={BATCH_SIZE}, OFFSET={real_offset}ï¼ˆå®é™…è¦è¯·æ±‚ {len(todo_words)} è¯ï¼‰")

    added_total = updated_total = 0
    failed_batches = []
    RUN_TS = datetime.now().strftime("%Y%m%d_%H%M%S")

    prog = tqdm(total=total_batches, desc="ä¸»æ‰¹æ¬¡")
    # å¹¶å‘æ‰§è¡Œ
    def submit_job(ex, key, idx, words):
        def job():
            ok, add, upd, used, reason = run_batch_adaptive(repo_dict, words, MAX_RETRY, build_prompt)
            with state_lock:
                if ok:
                    nonlocal added_total, updated_total
                    added_total += add; updated_total += upd
                    done_batches.add(key)
                    state["done_batches"] = sorted(list(done_batches))
                    save_state(state, RUN_ID)
            return (idx, ok, add, upd, used, reason)
        return ex.submit(job)

    with ThreadPoolExecutor(max_workers=WORKERS) as ex:
        futures = []
        for i, batch_words in enumerate(batches, 1):
            key = batch_key(batch_words)
            if key in done_batches:
                prog.update(1); continue
            futures.append(submit_job(ex, key, i, batch_words))  # ä¼  key

        for fut in as_completed(futures):
            i, ok, add, upd, used, reason = fut.result()
            if ok:
                print(f"SUCCESS batch {i}/{total_batches} | used {used} | added {add} | updated {upd} | total {_count_repo(repo_dict)}")
                prog.set_postfix({"total_words": _count_repo(repo_dict), "status": "ok"})
            else:
                print(f"FAILED batch {i}/{total_batches} (adaptive failed). Reason: {reason}")
                failed_batches.append({"words": batches[i-1], "reason": reason})
                prog.set_postfix({"total_words": _count_repo(repo_dict), "status": "fail"})
            prog.update(1)
    prog.close()

    # äºŒæ¬¡è¡¥æ•‘ï¼ˆå¯¹å¤±è´¥æ‰¹å„è‡ªå†è·‘ä¸€æ¬¡ï¼‰
    final_failed_words = []
    if failed_batches:
        print(f"ğŸ›  äºŒæ¬¡è¡¥æ•‘ï¼š{len(failed_batches)} æ‰¹ï¼Œå°æ‰¹é‡ {RETRY_BATCH_SIZE}")
        rbatches = [b["words"] for b in failed_batches]
        prog2 = tqdm(total=len(rbatches), desc="è¡¥æ•‘æ‰¹æ¬¡")
        for i, r_words in enumerate(rbatches, 1):
            # æŒ‰å°æ‰¹åˆ‡åˆ†
            chunks = split_batches(r_words, RETRY_BATCH_SIZE)
            ok_all = True
            for seg in chunks:
                ok, add, upd, used, reason = run_batch_adaptive(repo_dict, seg, RETRY_MAX_RETRY, build_rescue_prompt)
                if ok:
                    print(f"âœ… è¡¥æ•‘æ‰¹ {i}/{len(rbatches)} | æ–°å¢ {add} | æ›´æ–° {upd} | æ€» {_count_repo(repo_dict)}")
                else:
                    print(f"âŒ è¡¥æ•‘æ‰¹ {i}/{len(rbatches)} ä»å¤±è´¥ã€‚åŸå› ï¼š{reason}")
                    ok_all = False
            if not ok_all:
                final_failed_words.extend(r_words)
            prog2.update(1)
        prog2.close()

    # å†™å¤±è´¥æ—¥å¿—ï¼ˆä»…å½“ç¡®æœ‰å¤±è´¥ï¼‰
    if failed_batches:
        with open(os.path.join(LOG_DIR, f"failed_batches_{RUN_TS}.json"), "w", encoding="utf-8") as f:
            json.dump(failed_batches, f, ensure_ascii=False, indent=2)
    if final_failed_words:
        with open(os.path.join(LOG_DIR, f"failed_words_{RUN_TS}.txt"), "w", encoding="utf-8") as f:
            f.write("\n".join(sorted(set(final_failed_words))))

    # â€”â€” è‡ªåŠ¨æ‰§è¡Œ auto_whitelist.py + é‡æ–°è¡¥æ•‘ä¸€æ¬¡ï¼ˆä»…é’ˆå¯¹ä¸Šä¸€æ­¥è¿˜å¤±è´¥çš„è¯ï¼‰â€”â€”
    second_round_failed = []
    if final_failed_words:
        print("ğŸ¤– è§¦å‘è‡ªåŠ¨è±å…ï¼šè¿è¡Œ auto_whitelist.py â€¦â€¦")
        try:
            subprocess.run(["python", "auto_whitelist.py"], check=False)
        except Exception as e:
            print(f"è‡ªåŠ¨è±å…è„šæœ¬æ‰§è¡Œå¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{repr(e)}")
        # é‡æ–°åŠ è½½ç™½åå•
        global FUNCTION_WORDS
        FUNCTION_WORDS = load_function_words()
        # ä»…å¯¹â€œè¿˜å¤±è´¥çš„è¯â€å†è¡¥æ•‘ä¸€è½®
        print("ğŸ›  è‡ªåŠ¨è±å…åå†æ¬¡è¡¥æ•‘å¤±è´¥è¯â€¦â€¦")
        left = sorted(set([w.lower() for w in final_failed_words]))
        chunks = split_batches(left, min(RETRY_BATCH_SIZE, 5))
        for i, seg in enumerate(chunks, 1):
            ok, add, upd, used, reason = run_batch_adaptive(repo_dict, seg, RETRY_MAX_RETRY, build_rescue_prompt)
            if ok:
                print(f"âœ… äºŒæ¬¡åè¡¥æ•‘ {i}/{len(chunks)} | æ–°å¢ {add} | æ›´æ–° {upd}")
            else:
                print(f"âŒ äºŒæ¬¡åä»å¤±è´¥ {i}/{len(chunks)}ï¼š{reason}")
                second_round_failed.extend(seg)

    # æ³¨æ„ï¼šæ­¤æ—¶è¿˜æ²¡æœ‰second_round_failedæ•°æ®ï¼Œå°†åœ¨åé¢è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    
    # äºŒæ¬¡åä»å¤±è´¥ â†’ å†™å…¥æ°¸ä¹…å¤±è´¥æ–‡ä»¶ï¼Œå¹¶å¼ºåˆ¶æ¸…ç†ä¸´æ—¶å¤±è´¥æ—¥å¿—
    if second_round_failed:
        print(f"ğŸ“Œ äºŒæ¬¡åä»å¤±è´¥ {len(second_round_failed)} ä¸ªï¼Œå†™å…¥ {PERM_FAILED_FILE} å¹¶æ¸…ç†ä¸´æ—¶å¤±è´¥æ—¥å¿—")
        # åˆå¹¶åˆ°æ°¸ä¹…æ–‡ä»¶ï¼ˆå»é‡ï¼‰
        exist = set()
        if os.path.exists(PERM_FAILED_FILE):
            try:
                exist = set(json.load(open(PERM_FAILED_FILE, "r", encoding="utf-8")))
            except:
                exist = set()
        merged = sorted(exist.union({w.lower() for w in second_round_failed}))
        with open(PERM_FAILED_FILE, "w", encoding="utf-8") as pf:
            json.dump(merged, pf, ensure_ascii=False, indent=2)
        # å¼ºåˆ¶æ¸…ç†ä¸´æ—¶å¤±è´¥æ—¥å¿—
        for pat in ("failed_words_*.txt", "failed_batches_*.json"):
            for p in glob.glob(os.path.join(LOG_DIR, pat)):
                try: os.remove(p)
                except: pass
        final_failed_words = []  # è§†ä¸ºå·²å¤„ç†ï¼ˆè½¬å…¥æ°¸ä¹…æ¸…å•ï¼‰
        failed_batches = []

    # è¡¥é½ IPA & ä¾‹å¥
    print("ğŸ”§ æ­£åœ¨è¡¥é½ IPA ä¸ ä¾‹å¥...")
    all_items = list(repo_dict.values())
    enrich_with_ipa_and_sentences(all_items)
    # åˆå¹¶è½ç›˜
    with repo_lock:
        for it in all_items:
            repo_dict[(it.get("word") or "").lower()] = it
        _save_repo(repo_dict)

    # è¿è¡Œæ€»ç»“ï¼ˆè®¡ç®—æœ€ç»ˆå‡†ç¡®ç»Ÿè®¡ï¼‰
    current_total = _count_repo(repo_dict)
    net_added = current_total - initial_total  # å‡€å¢é•¿ï¼ˆçœŸå®çš„è¯åº“å¢é•¿ï¼‰
    
    stats_failed_batches_count = len([b for b in failed_batches])
    stats_final_failed_words_count = len(final_failed_words) 
    stats_second_round_failed_count = len(second_round_failed)
    
    # æ€»å¤±è´¥è¯æ•° = æœ€ç»ˆå†™å…¥æ°¸ä¹…å¤±è´¥æ–‡ä»¶çš„è¯æ•°ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    total_failed_words = stats_second_round_failed_count
    
    summary = (f"ğŸ¯ ç›®æ ‡ {TOTAL_WORDS} è¯ | ä¸»æ‰¹ {total_batches} | å¤±è´¥ä¸»æ‰¹ {stats_failed_batches_count} | "
               f"å¤±è´¥è¯ {total_failed_words} | æ°¸ä¹…å¤±è´¥ {stats_second_round_failed_count} | "
               f"æ“ä½œ {added_total}æ–°å¢+{updated_total}æ›´æ–° | å‡€å¢ {net_added} | è¯åº“æ€»æ•° {current_total}")
    print(summary)

    # å†™ run æ¦‚è§ˆæ—¥å¿—
    with open(os.path.join(LOG_DIR, f"run_{RUN_TS}.log"), "w", encoding="utf-8") as lf:
        lf.write(summary + "\n")

    # æ™ºèƒ½æ¸…ç†ï¼ˆåŸºäºçœŸå®çš„å¤±è´¥æƒ…å†µåˆ¤æ–­ï¼‰
    is_complete_success = (stats_failed_batches_count == 0) and (total_failed_words == 0)
    if is_complete_success:
        if LOG_CLEAN_ON_SUCCESS:
            removed = cleanup_logs(repo_dict, clean_all_if_success=True, prune_resolved=False)
            if removed: print(f"ğŸ§¹ å·²æ¸…ç†å†å²å¤±è´¥æ—¥å¿—ï¼š{removed}")
    else:
        if LOG_PRUNE_RESOLVED:
            removed = cleanup_logs(repo_dict, clean_all_if_success=False, prune_resolved=True)
            if removed: print(f"ğŸ§¹ å·²æ¸…ç†å·²è¡¥é½çš„å¤±è´¥æ—¥å¿—ï¼š{removed}")

    prune_run_logs()

if __name__ == "__main__":
    main()
