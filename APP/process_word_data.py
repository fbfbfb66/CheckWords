#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯æ±‡æ•°æ®é¢„å¤„ç†è„šæœ¬
é€šç”¨å¤„ç†è„šæœ¬ï¼Œæ”¯æŒå¤„ç†ä»»æ„æ ¼å¼çš„è¯æ±‡JSONæ–‡ä»¶
"""

import json
import sqlite3
import os
import glob
from typing import Dict, List, Any

def replace_fran(text: str) -> str:
    """
    æ›¿æ¢æ³•è¯­å­—æ¯ä¸ºè‹±è¯­å­—æ¯
    """
    if not text:
        return text

    fr_en = [
        ['Ã©', 'e'], ['Ãª', 'e'], ['Ã¨', 'e'], ['Ã«', 'e'],
        ['Ã ', 'a'], ['Ã¢', 'a'], ['Ã§', 'c'], ['Ã®', 'i'],
        ['Ã¯', 'i'], ['Ã´', 'o'], ['Ã¹', 'u'], ['Ã»', 'u'],
        ['Ã¼', 'u'], ['Ã¿', 'y']
    ]

    for fr_char, en_char in fr_en:
        text = text.replace(fr_char, en_char)

    return text

def extract_word_data(word_json: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»åŸå§‹JSONä¸­æå–æ‰€éœ€çš„å­—æ®µ
    æ ¹æ®Requirements.mdä¸­å®šä¹‰çš„æ•°æ®åº“æ¶æ„
    """
    try:
        word_content = word_json.get('content', {}).get('word', {})
        content = word_content.get('content', {})

        # åŸºæœ¬ä¿¡æ¯
        word_rank = word_json.get('wordRank', 0)
        head_word = word_json.get('headWord', '')
        word_id = word_content.get('wordId', '')
        book_id = word_json.get('bookId', '')

        # æ¸…ç†æ³•è¯­å­—æ¯
        head_word = replace_fran(head_word)

        # éŸ³æ ‡
        usphone = replace_fran(content.get('usphone', ''))
        ukphone = replace_fran(content.get('ukphone', ''))
        usspeech = content.get('usspeech', '')
        ukspeech = content.get('ukspeech', '')

        # é‡Šä¹‰
        trans_data = content.get('trans', [])
        trans = json.dumps(trans_data, ensure_ascii=False)

        # ä¾‹å¥
        sentences_data = content.get('sentence', {}).get('sentences', [])
        # æ¸…ç†ä¾‹å¥ä¸­çš„æ³•è¯­å­—æ¯
        for sentence in sentences_data:
            if 'sContent' in sentence:
                sentence['sContent'] = replace_fran(sentence['sContent'])
            if 'sCn' in sentence:
                sentence['sCn'] = replace_fran(sentence['sCn'])
        sentences = json.dumps(sentences_data, ensure_ascii=False)

        # çŸ­è¯­
        phrases_data = content.get('phrase', {}).get('phrases', [])
        # æ¸…ç†çŸ­è¯­ä¸­çš„æ³•è¯­å­—æ¯
        for phrase in phrases_data:
            if 'pContent' in phrase:
                phrase['pContent'] = replace_fran(phrase['pContent'])
            if 'pCn' in phrase:
                phrase['pCn'] = replace_fran(phrase['pCn'])
        phrases = json.dumps(phrases_data, ensure_ascii=False)

        # åŒè¿‘ä¹‰è¯
        synos_data = content.get('syno', {}).get('synos', [])
        # æ¸…ç†åŒè¿‘ä¹‰è¯ä¸­çš„æ³•è¯­å­—æ¯
        for syno in synos_data:
            if 'tran' in syno:
                syno['tran'] = replace_fran(syno['tran'])
            if 'hwds' in syno:
                for hwd in syno['hwds']:
                    if 'w' in hwd:
                        hwd['w'] = replace_fran(hwd['w'])
        synonyms = json.dumps(synos_data, ensure_ascii=False)

        # åŒæ ¹è¯
        rel_words_data = content.get('relWord', {}).get('rels', [])
        # æ¸…ç†åŒæ ¹è¯ä¸­çš„æ³•è¯­å­—æ¯
        for rel_word in rel_words_data:
            if 'words' in rel_word:
                for word_item in rel_word['words']:
                    if 'hwd' in word_item:
                        word_item['hwd'] = replace_fran(word_item['hwd'])
                    if 'tran' in word_item:
                        word_item['tran'] = replace_fran(word_item['tran'])
        rel_words = json.dumps(rel_words_data, ensure_ascii=False)

        # æµ‹è¯•é¢˜
        exams_data = content.get('exam', [])
        # æ¸…ç†æµ‹è¯•é¢˜ä¸­çš„æ³•è¯­å­—æ¯
        for exam in exams_data:
            if 'question' in exam:
                exam['question'] = replace_fran(exam['question'])
            if 'choices' in exam:
                for choice in exam['choices']:
                    if 'choice' in choice:
                        choice['choice'] = replace_fran(choice['choice'])
            if 'answer' in exam and 'explain' in exam['answer']:
                exam['answer']['explain'] = replace_fran(exam['answer']['explain'])
        exams = json.dumps(exams_data, ensure_ascii=False)

        # åˆ†ç±»ç›´æ¥ä½¿ç”¨bookId
        category = book_id

        # æœç´¢å†…å®¹ (ç”¨äºå…¨æ–‡æœç´¢)
        search_parts = [head_word]

        # æ·»åŠ é‡Šä¹‰åˆ°æœç´¢å†…å®¹
        for trans_item in trans_data:
            if 'tranCn' in trans_item:
                search_parts.append(trans_item['tranCn'])

        # æ·»åŠ ä¾‹å¥åˆ°æœç´¢å†…å®¹
        for sentence in sentences_data:
            if 'sContent' in sentence:
                search_parts.append(sentence['sContent'])
            if 'sCn' in sentence:
                search_parts.append(sentence['sCn'])

        search_content = ' '.join(search_parts)
        search_content = replace_fran(search_content)

        return {
            'wordRank': word_rank,
            'word': head_word,
            'wordId': word_id,
            'usphone': usphone,
            'ukphone': ukphone,
            'usspeech': usspeech,
            'ukspeech': ukspeech,
            'trans': trans,
            'sentences': sentences,
            'phrases': phrases,
            'synonyms': synonyms,
            'relWords': rel_words,
            'exams': exams,
            'category': category,
            'bookId': book_id,
            'searchContent': search_content
        }

    except Exception as e:
        print(f"æå–æ•°æ®æ—¶å‡ºé”™: {e}")
        print(f"é—®é¢˜æ•°æ®: {word_json}")
        return None

def get_database_name_from_bookid(book_id: str) -> str:
    """
    æ ¹æ®bookIdç”Ÿæˆæ•°æ®åº“æ–‡ä»¶å
    ç›´æ¥ä½¿ç”¨bookIdä½œä¸ºæ–‡ä»¶åï¼Œä¿æŒä¸€è‡´æ€§
    """
    import re
    # æ¸…ç†bookIdä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
    clean_book_id = re.sub(r'[^\w]', '_', book_id)
    return f"{clean_book_id}.db"

def create_database(db_path: str):
    """
    åˆ›å»ºSQLiteæ•°æ®åº“å’Œè¡¨
    æ ¹æ®Requirements.mdä¸­çš„æ•°æ®åº“æ¶æ„
    """
    # å¦‚æœæ•°æ®åº“å·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
    if os.path.exists(db_path):
        os.remove(db_path)

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # åˆ›å»ºwordsè¡¨
    cursor.execute('''
        CREATE TABLE words_table (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            wordRank INTEGER NOT NULL,
            headWord TEXT NOT NULL,
            wordId TEXT NOT NULL UNIQUE,
            usphone TEXT,
            ukphone TEXT,
            usspeech TEXT,
            ukspeech TEXT,
            trans TEXT DEFAULT '[]',
            sentences TEXT DEFAULT '[]',
            phrases TEXT DEFAULT '[]',
            synonyms TEXT DEFAULT '[]',
            relWords TEXT DEFAULT '[]',
            exams TEXT DEFAULT '[]',
            category TEXT NOT NULL,
            bookId TEXT NOT NULL,
            searchContent TEXT NOT NULL,
            createdAt DATETIME DEFAULT CURRENT_TIMESTAMP,
            updatedAt DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    # åˆ›å»ºFTS5è™šæ‹Ÿè¡¨ç”¨äºå…¨æ–‡æœç´¢
    cursor.execute('''
        CREATE VIRTUAL TABLE words_fts USING fts5(
            headWord,
            searchContent,
            content='words_table',
            content_rowid='id'
        )
    ''')

    # åˆ›å»ºç´¢å¼•
    cursor.execute('CREATE INDEX idx_headWord ON words_table(headWord)')
    cursor.execute('CREATE INDEX idx_category ON words_table(category)')
    cursor.execute('CREATE INDEX idx_wordId ON words_table(wordId)')
    cursor.execute('CREATE INDEX idx_wordRank ON words_table(wordRank)')

    conn.commit()
    conn.close()

    print(f"æ•°æ®åº“ {db_path} åˆ›å»ºæˆåŠŸ")

def import_data_to_database(filename: str, db_path: str):
    """
    ä»JSONæ–‡ä»¶å¯¼å…¥æ•°æ®åˆ°æŒ‡å®šæ•°æ®åº“
    """
    if not os.path.exists(filename):
        print(f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        return 0, 0

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    processed_count = 0
    error_count = 0

    try:
        with open(filename, 'r', encoding='utf-8') as file:
            for line_num, line in enumerate(file, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    # è§£æJSON
                    word_json = json.loads(line)

                    # æå–æ•°æ®
                    word_data = extract_word_data(word_json)

                    if word_data:
                        # æ’å…¥æ•°æ®åº“
                        cursor.execute('''
                            INSERT INTO words_table (
                                wordRank, headWord, wordId, usphone, ukphone,
                                usspeech, ukspeech, trans, sentences, phrases,
                                synonyms, relWords, exams, category, bookId,
                                searchContent
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            word_data['wordRank'],
                            word_data['word'],
                            word_data['wordId'],
                            word_data['usphone'],
                            word_data['ukphone'],
                            word_data['usspeech'],
                            word_data['ukspeech'],
                            word_data['trans'],
                            word_data['sentences'],
                            word_data['phrases'],
                            word_data['synonyms'],
                            word_data['relWords'],
                            word_data['exams'],
                            word_data['category'],
                            word_data['bookId'],
                            word_data['searchContent']
                        ))

                        # æ’å…¥FTSè¡¨
                        cursor.execute('''
                            INSERT INTO words_fts (headWord, searchContent)
                            VALUES (?, ?)
                        ''', (
                            word_data['word'],
                            word_data['searchContent']
                        ))

                        processed_count += 1

                        # æ¯1000æ¡æäº¤ä¸€æ¬¡
                        if processed_count % 1000 == 0:
                            conn.commit()
                            print(f"å·²å¤„ç† {processed_count} æ¡æ•°æ®...")

                except json.JSONDecodeError as e:
                    print(f"ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
                    error_count += 1
                except Exception as e:
                    print(f"ç¬¬ {line_num} è¡Œå¤„ç†é”™è¯¯: {e}")
                    error_count += 1

    except Exception as e:
        print(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")

    finally:
        conn.commit()
        conn.close()

    print(f"æ•°æ®å¯¼å…¥å®Œæˆ!")
    print(f"æˆåŠŸå¤„ç†: {processed_count} æ¡")
    print(f"é”™è¯¯æ•°é‡: {error_count} æ¡")

    return processed_count, error_count

def create_main_database():
    """
    åˆ›å»ºæˆ–åˆå§‹åŒ–æ±‡æ€»æ•°æ®åº“
    å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œå­˜åœ¨åˆ™ä¿ç•™ç°æœ‰æ•°æ®
    """
    db_path = 'assets/data/main.db'

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='words_table'")
    table_exists = cursor.fetchone() is not None

    if not table_exists:
        # åˆ›å»ºwordsè¡¨
        cursor.execute('''
            CREATE TABLE words_table (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                wordRank INTEGER NOT NULL,
                headWord TEXT NOT NULL,
                wordId TEXT NOT NULL,
                usphone TEXT,
                ukphone TEXT,
                usspeech TEXT,
                ukspeech TEXT,
                trans TEXT DEFAULT '[]',
                sentences TEXT DEFAULT '[]',
                phrases TEXT DEFAULT '[]',
                synonyms TEXT DEFAULT '[]',
                relWords TEXT DEFAULT '[]',
                exams TEXT DEFAULT '[]',
                category TEXT NOT NULL,
                bookId TEXT NOT NULL,
                searchContent TEXT NOT NULL,
                createdAt DATETIME DEFAULT CURRENT_TIMESTAMP,
                updatedAt DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        print("åˆ›å»ºwords_tableè¡¨")

    # æ£€æŸ¥FTSè¡¨æ˜¯å¦å­˜åœ¨
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='words_fts'")
    fts_exists = cursor.fetchone() is not None

    if not fts_exists:
        # åˆ›å»ºFTS5è™šæ‹Ÿè¡¨ç”¨äºå…¨æ–‡æœç´¢
        cursor.execute('''
            CREATE VIRTUAL TABLE words_fts USING fts5(
                headWord,
                searchContent,
                content='words_table',
                content_rowid='id'
            )
        ''')
        print("åˆ›å»ºwords_ftså…¨æ–‡æœç´¢è¡¨")

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
    cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name='idx_headWord'")
    if cursor.fetchone() is None:
        cursor.execute('CREATE INDEX idx_headWord ON words_table(headWord)')
        print("åˆ›å»ºidx_headWordç´¢å¼•")

    cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name='idx_category'")
    if cursor.fetchone() is None:
        cursor.execute('CREATE INDEX idx_category ON words_table(category)')
        print("åˆ›å»ºidx_categoryç´¢å¼•")

    cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name='idx_wordId'")
    if cursor.fetchone() is None:
        cursor.execute('CREATE INDEX idx_wordId ON words_table(wordId)')
        print("åˆ›å»ºidx_wordIdç´¢å¼•")

    cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name='idx_wordRank'")
    if cursor.fetchone() is None:
        cursor.execute('CREATE INDEX idx_wordRank ON words_table(wordRank)')
        print("åˆ›å»ºidx_wordRankç´¢å¼•")

    conn.commit()
    conn.close()

    if table_exists:
        print(f"æ±‡æ€»æ•°æ®åº“ {db_path} å·²å­˜åœ¨ï¼Œä¿ç•™ç°æœ‰æ•°æ®")
    else:
        print(f"æ±‡æ€»æ•°æ®åº“ {db_path} åˆ›å»ºæˆåŠŸ")

def merge_to_main_database(source_db_path: str):
    """
    å°†æºæ•°æ®åº“çš„æ•°æ®åˆå¹¶åˆ°æ±‡æ€»æ•°æ®åº“ï¼ˆå»é‡ï¼‰
    å»é‡é€»è¾‘ï¼šåŸºäºheadWordå»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•
    """
    main_db_path = 'assets/data/main.db'

    if not os.path.exists(source_db_path):
        print(f"æºæ•°æ®åº“ {source_db_path} ä¸å­˜åœ¨")
        return 0, 0

    # è¿æ¥æºæ•°æ®åº“å’Œæ±‡æ€»æ•°æ®åº“
    source_conn = sqlite3.connect(source_db_path)
    main_conn = sqlite3.connect(main_db_path)

    source_cursor = source_conn.cursor()
    main_cursor = main_conn.cursor()

    try:
        # è·å–æºæ•°æ®åº“çš„æ‰€æœ‰æ•°æ®
        source_cursor.execute('SELECT * FROM words_table ORDER BY wordRank')
        source_words = source_cursor.fetchall()

        if not source_words:
            print("æºæ•°æ®åº“ä¸ºç©º")
            return 0, 0

        # è·å–åˆ—å
        column_names = [description[0] for description in source_cursor.description]

        processed_count = 0
        skipped_count = 0

        for word_row in source_words:
            # å°†å…ƒç»„è½¬æ¢ä¸ºå­—å…¸
            word_dict = dict(zip(column_names, word_row))
            head_word = word_dict['headWord']

            # æ£€æŸ¥æ±‡æ€»æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥å•è¯
            main_cursor.execute('SELECT COUNT(*) FROM words_table WHERE headWord = ?', (head_word,))
            count = main_cursor.fetchone()[0]

            if count == 0:
                # æ’å…¥åˆ°æ±‡æ€»æ•°æ®åº“
                insert_columns = list(word_dict.keys())[1:]  # è·³è¿‡idå­—æ®µ
                insert_values = list(word_dict.values())[1:]  # è·³è¿‡idå­—æ®µ

                placeholders = ', '.join(['?' for _ in insert_columns])
                columns_str = ', '.join(insert_columns)

                main_cursor.execute(f'''
                    INSERT INTO words_table ({columns_str})
                    VALUES ({placeholders})
                ''', insert_values)

                # æ’å…¥FTSè¡¨
                main_cursor.execute('''
                    INSERT INTO words_fts (headWord, searchContent)
                    VALUES (?, ?)
                ''', (head_word, word_dict['searchContent']))

                processed_count += 1
            else:
                skipped_count += 1

        main_conn.commit()

        print(f"åˆå¹¶åˆ°æ±‡æ€»æ•°æ®åº“å®Œæˆ!")
        print(f"æ–°å¢è®°å½•: {processed_count} æ¡")
        print(f"è·³è¿‡é‡å¤: {skipped_count} æ¡")

        return processed_count, skipped_count

    except Exception as e:
        print(f"åˆå¹¶æ•°æ®æ—¶å‡ºé”™: {e}")
        return 0, 0

    finally:
        source_conn.close()
        main_conn.close()

def find_json_files():
    """
    åœ¨assets/data/jsonç›®å½•ä¸‹æŸ¥æ‰¾JSONæ–‡ä»¶
    """
    json_dir = 'assets/data/json'

    if not os.path.exists(json_dir):
        print(f"ç›®å½• {json_dir} ä¸å­˜åœ¨")
        return []

    # æŸ¥æ‰¾æ‰€æœ‰.jsonæ–‡ä»¶
    json_files = glob.glob(os.path.join(json_dir, '*.json'))

    return json_files

def get_book_id_from_json(filename: str) -> str:
    """
    ä»JSONæ–‡ä»¶ä¸­è¯»å–ç¬¬ä¸€ä¸ªæ¡ç›®è·å–bookId
    """
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip()
                if line:
                    word_json = json.loads(line)
                    book_id = word_json.get('bookId', '')
                    if book_id:
                        return book_id
    except Exception as e:
        print(f"è¯»å–bookIdæ—¶å‡ºé”™: {e}")

    return 'unknown'

def backup_existing_data():
    """
    å¤‡ä»½ç°æœ‰çš„æ±‡æ€»æ•°æ®åº“
    """
    main_db_path = 'assets/data/main.db'
    backup_path = 'assets/data/main.db.backup'

    if os.path.exists(main_db_path):
        try:
            # å¦‚æœå¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if os.path.exists(backup_path):
                os.remove(backup_path)
            # å¤åˆ¶æ–‡ä»¶ä½œä¸ºå¤‡ä»½
            import shutil
            shutil.copy2(main_db_path, backup_path)
            print(f"å·²å¤‡ä»½ç°æœ‰æ•°æ®åº“åˆ°: {backup_path}")
            return True
        except Exception as e:
            print(f"å¤‡ä»½æ•°æ®åº“å¤±è´¥: {e}")
            return False
    return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹å¤„ç†è¯æ±‡æ•°æ®...")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('assets/data/db', exist_ok=True)
    os.makedirs('assets/data/json', exist_ok=True)

    # æŸ¥æ‰¾JSONæ–‡ä»¶
    json_files = find_json_files()

    if not json_files:
        print("æœªæ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶ï¼Œè¯·å°†è¯æ±‡æ•°æ®æ–‡ä»¶æ”¾åœ¨ assets/data/json/ ç›®å½•ä¸‹")
        return

    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

    # å¤‡ä»½ç°æœ‰æ•°æ®
    backup_existing_data()

    # åˆ›å»ºæ±‡æ€»æ•°æ®åº“ï¼ˆä¿ç•™ç°æœ‰æ•°æ®ï¼‰
    print("åˆå§‹åŒ–æ±‡æ€»æ•°æ®åº“...")
    create_main_database()

    total_processed = 0
    total_merged = 0

    # å¤„ç†æ¯ä¸ªJSONæ–‡ä»¶
    for json_file in json_files:
        print(f"\nå¤„ç†æ–‡ä»¶: {json_file}")

        # è·å–bookId
        book_id = get_book_id_from_json(json_file)
        print(f"æ£€æµ‹åˆ°bookId: {book_id}")

        # ç”Ÿæˆæ•°æ®åº“æ–‡ä»¶å
        db_filename = get_database_name_from_bookid(book_id)
        db_path = f'assets/data/db/{db_filename}'

        print(f"ç”Ÿæˆç‹¬ç«‹æ•°æ®åº“: {db_path}")

        # åˆ›å»ºç‹¬ç«‹æ•°æ®åº“
        create_database(db_path)

        # å¯¼å…¥æ•°æ®åˆ°ç‹¬ç«‹æ•°æ®åº“
        print(f"å¯¼å…¥æ•°æ®åˆ° {db_path}...")
        processed_count, error_count = import_data_to_database(json_file, db_path)
        total_processed += processed_count

        if processed_count > 0:
            # åˆå¹¶åˆ°æ±‡æ€»æ•°æ®åº“
            print(f"åˆå¹¶æ•°æ®åˆ°æ±‡æ€»æ•°æ®åº“...")
            merged_count, skipped_count = merge_to_main_database(db_path)
            total_merged += merged_count
        else:
            print("æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡åˆå¹¶")

    print(f"\nå¤„ç†å®Œæˆ!")
    print(f"æ€»è®¡å¤„ç†: {total_processed} æ¡è¯æ±‡")
    print(f"æ€»è®¡åˆå¹¶åˆ°æ±‡æ€»æ•°æ®åº“: {total_merged} æ¡è¯æ±‡")
    print("ç‹¬ç«‹æ•°æ®åº“æ–‡ä»¶ä½äº: assets/data/db/")
    print("æ±‡æ€»æ•°æ®åº“æ–‡ä»¶ä½äº: assets/data/main.db")

    # è‡ªåŠ¨å‡çº§ç‰ˆæœ¬å·
    if total_merged > 0:
        print("\nğŸ”„ æ­£åœ¨è‡ªåŠ¨å‡çº§åº”ç”¨ç‰ˆæœ¬å·...")
        new_version = update_version_number()
        if new_version:
            print(f"ğŸš€ ä¸‹æ¬¡å¯åŠ¨åº”ç”¨æ—¶å°†è‡ªåŠ¨å¯¼å…¥æ–°æ•°æ®ï¼")
            print(f"ğŸ“± è¯·é‡å¯åº”ç”¨ä»¥ä½¿ç”¨æ–°çš„è¯æ±‡æ•°æ®ã€‚")
        else:
            print("âš ï¸  ç‰ˆæœ¬å·å‡çº§å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹Dartæ–‡ä»¶ä¸­çš„ç‰ˆæœ¬å·")
    else:
        print("\nğŸ’¡ æ²¡æœ‰æ–°æ•°æ®åˆå¹¶ï¼Œç‰ˆæœ¬å·ä¿æŒä¸å˜")

def update_version_number():
    """
    è‡ªåŠ¨å‡çº§Dartæ–‡ä»¶ä¸­çš„ç‰ˆæœ¬å·
    """
    dart_file_path = 'lib/core/services/data_import_service.dart'

    try:
        with open(dart_file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        # æŸ¥æ‰¾å½“å‰ç‰ˆæœ¬å·
        import re
        version_pattern = r'static const int _currentImportVersion = (\d+);'
        match = re.search(version_pattern, content)

        if match:
            current_version = int(match.group(1))
            new_version = current_version + 1

            # æ›¿æ¢ç‰ˆæœ¬å·
            new_content = re.sub(
                version_pattern,
                f'static const int _currentImportVersion = {new_version};',
                content
            )

            # ä¿å­˜æ–‡ä»¶
            with open(dart_file_path, 'w', encoding='utf-8') as file:
                file.write(new_content)

            print(f"âœ… ç‰ˆæœ¬å·å·²è‡ªåŠ¨å‡çº§: {current_version} â†’ {new_version}")
            print(f"ğŸ“ å·²æ›´æ–°æ–‡ä»¶: {dart_file_path}")
            return new_version
        else:
            print("âŒ æœªæ‰¾åˆ°ç‰ˆæœ¬å·å®šä¹‰ï¼Œè¯·æ£€æŸ¥Dartæ–‡ä»¶")
            return None

    except Exception as e:
        print(f"âŒ æ›´æ–°ç‰ˆæœ¬å·å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()