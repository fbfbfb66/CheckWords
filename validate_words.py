#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
validate_words.py - ç‹¬ç«‹çš„è¯æ±‡éªŒè¯è„šæœ¬

åŠŸèƒ½ï¼š
- è¯»å– temp_words.json ä¸­çš„AIç”Ÿæˆç»“æœ
- ä½¿ç”¨ä¸ Resource/pipeline.py ç›¸åŒçš„éªŒè¯é€»è¾‘
- æ£€æŸ¥JSON Schemaã€è´¨é‡æ£€æŸ¥ã€è¦†ç›–ç‡ç­‰
- è¾“å‡ºè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
"""

import os
import json
from typing import List, Dict, Any, Tuple

# ============ Schema å®šä¹‰ï¼ˆä¸ pipeline.py ä¿æŒä¸€è‡´ï¼‰ ============
SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "required": ["word", "parts_of_speech", "pos_meanings", "phrases", "sentences", "pronunciation"],
        "properties": {
            "word": {"type": "string"},
            "parts_of_speech": {
                "type": "array",
                "items": {"type": "string"},
                "minItems": 1
            },
            "pos_meanings": {
                "type": "array",
                "items": {
                    "type": "object",
                    "required": ["pos", "cn"],
                    "properties": {
                        "pos": {"type": "string"},
                        "cn":  {"type": "string"}
                    }
                },
                "minItems": 1
            },
            "phrases": {
                "type": "array",
                "items": {
                    "type": "object",
                    "required": ["phrase", "dialog"],
                    "properties": {
                        "phrase": {"type": "string"},
                        "dialog": {
                            "type": "object",
                            "required": ["A", "B"],
                            "properties": {
                                "A": {"type": "string"},
                                "B": {"type": "string"}
                            }
                        }
                    }
                }
            },
            "sentences": {
                "type": "array",
                "items": {
                    "type": "object",
                    "required": ["en", "cn"],
                    "properties": {
                        "en": {"type": "string"},
                        "cn": {"type": "string"}
                    }
                }
            },
            "pronunciation": {
                "type": "object",
                "required": ["US"],
                "properties": {
                    "US": {
                        "type": "object",
                        "required": ["ipa", "audio"],
                        "properties": {
                            "ipa": {"type": ["string", "null"]},
                            "audio": {"type": ["string", "null"]}
                        }
                    }
                }
            }
        }
    }
}

# ============ åŠŸèƒ½è¯ç™½åå•ï¼ˆä¸ pipeline.py ä¿æŒä¸€è‡´ï¼‰ ============
BASE_FUNCTION_WORDS = {
    "a","an","the","some","any","each","every","either","neither","another",
    "this","that","these","those","both","all","half","many","much","few","little",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","her","its","our","their","mine","yours","hers","ours","theirs",
    "someone","anyone","everyone","no one","something","anything","everything","nothing",
    "to","of","in","on","at","for","from","by","with","about","as","after","before",
    "between","around","since","than","over","under","into","onto","through","during",
    "without","within","across","against","toward","upon","off","up","down","out",
    "and","or","but","if","so","yet","nor","though","although","because","while","unless",
    "be","am","is","are","was","were","been","being",
    "do","does","did","done","doing",
    "have","has","had","having",
    "can","could","may","might","must","shall","should","will","would",
    "not","n't","no","only","very","just",
    "i'm","you're","he's","she's","it's","we're","they're",
    "i've","you've","we've","they've",
    "i'd","you'd","he'd","she'd","we'd","they'd",
    "i'll","you'll","he'll","she'll","we'll","they'll",
    "don't","doesn't","didn't","can't","couldn't","won't","wouldn't","isn't",
    "aren't","wasn't","weren't","haven't","hasn't","hadn't","shouldn't","mustn't",
    "he's","there's","they'll","however","less","tired"  # æ·»åŠ å¤±è´¥è¯ä¾›æµ‹è¯•
}

CONTENT_BLACKLIST = {
    "help","find","show","make","take","give","get","go","come","look","work","call",
    "use","need","try","keep","think","know","feel","tell","ask","play","run","write",
    "people","person","thing","time","way","good","new","old","high","big","small",
    "better","best","always","never","end","begin","start"
}

FUNCTION_WORDS_FILE = "function_words_custom.txt"

def load_function_words():
    """åŠ è½½åŠŸèƒ½è¯ç™½åå•"""
    words = set(BASE_FUNCTION_WORDS)
    if os.path.exists(FUNCTION_WORDS_FILE):
        try:
            with open(FUNCTION_WORDS_FILE, "r", encoding="utf-8") as f:
                for line in f:
                    w = line.strip().lower()
                    if w and w not in CONTENT_BLACKLIST:
                        words.add(w)
        except Exception as e:
            print(f"âš ï¸ è¯»å– {FUNCTION_WORDS_FILE} å¤±è´¥ï¼š{e}")
    return words

FUNCTION_WORDS = load_function_words()

# ============ éªŒè¯å‡½æ•°ï¼ˆä¸ pipeline.py ä¿æŒä¸€è‡´ï¼‰ ============
def _normalize_pos_list(pos_list):
    """è§„èŒƒåŒ–è¯æ€§åˆ—è¡¨"""
    return [x.strip() for x in pos_list if x and x.strip()]

def _normalize_pos_meanings(pm_list):
    """è§„èŒƒåŒ–pos_meaningsåˆ—è¡¨"""
    res = []
    for pm in pm_list:
        if isinstance(pm, dict) and "pos" in pm and "cn" in pm:
            pos = pm["pos"].strip() if pm["pos"] else ""
            cn = pm["cn"].strip() if pm["cn"] else ""
            if pos and cn:
                res.append({"pos": pos, "cn": cn})
    return res

def check_pos_alignment(item):
    """ç¡®ä¿ parts_of_speech ä¸ pos_meanings ä¸€ä¸€å¯¹åº”"""
    pos_list = _normalize_pos_list(item.get("parts_of_speech", []))
    pm_list = _normalize_pos_meanings(item.get("pos_meanings", []))
    if not pos_list or not pm_list:
        return False, "pos-empty"
    pos_set = set(pos_list)
    pm_pos = {x["pos"] for x in pm_list}
    if pos_set != pm_pos:
        return False, f"pos-mismatch: pos={sorted(pos_set)} pm={sorted(pm_pos)}"
    return True, ""

def quality_checks(items):
    """è´¨é‡æ£€æŸ¥ï¼ˆä¸ pipeline.py ä¿æŒä¸€è‡´ï¼‰"""
    bad = []
    for it in items:
        w = (it.get("word") or "").lower().strip() or "?"
        
        # === åŠŸèƒ½è¯è±å…ï¼šåªéœ€åŸºæœ¬posæ£€æŸ¥ï¼Œæ— éœ€phrase/dialog ===
        if w in FUNCTION_WORDS:
            # å¯¹åŠŸèƒ½è¯ä½¿ç”¨å®½æ¾çš„posæ£€æŸ¥
            pos_list = _normalize_pos_list(it.get("parts_of_speech", []))
            pm_list = _normalize_pos_meanings(it.get("pos_meanings", []))
            # åŠŸèƒ½è¯åªè¦æœ‰ä»»ä½•posä¿¡æ¯å°±é€šè¿‡
            if not pos_list and not pm_list:
                bad.append((w, "function-word-pos-empty"))
                continue
            # åŠŸèƒ½è¯ä¸è¦æ±‚posä¸pos_meaningsä¸¥æ ¼å¯¹é½ï¼Œåªè¦ä¸ä¸ºç©ºå³å¯
            continue
        
        # 1) éåŠŸèƒ½è¯çš„ä¸¥æ ¼poså¯¹é½æ£€æŸ¥
        ok, reason = check_pos_alignment(it)
        if not ok:
            bad.append((w, reason))
            continue

        phr = it.get("phrases", [])

        # éåŠŸèƒ½è¯å…è®¸ç©ºphrasesï¼ˆå½“æ‰¾ä¸åˆ°åˆé€‚çœŸå®çŸ­è¯­æ—¶ï¼‰
        # å¦‚æœæœ‰phrasesï¼Œåˆ™å¿…é¡»æ˜¯çœŸå®çŸ­è¯­ï¼Œä¸èƒ½æ˜¯å•è¯æœ¬ä½“

        # 2) phrase å¿…é¡»åœ¨ A/B å‡ºç°ï¼ˆå®½æ¾ï¼‰ï¼›æˆ–è‡³å°‘ word æœ¬ä½“å‡ºç°
        ok_all = True
        for p in phr:
            phrase = (p.get("phrase") or "").strip()
            d = p.get("dialog", {}) or {}
            
            if not phrase:
                bad.append((w, "phrase-empty"))
                ok_all = False
                break
                
            # æ£€æŸ¥çŸ­è¯­ä¸èƒ½ç­‰äºå•è¯æœ¬ä½“
            if phrase.lower() == w.lower():
                bad.append((w, "phrase-equals-word"))
                ok_all = False
                break
            
            a = (d.get("A") or "").strip()
            b = (d.get("B") or "").strip()
            
            if not a or not b:
                bad.append((w, "dialog-empty"))
                ok_all = False
                break
            
            # æ£€æŸ¥å¯¹è¯ä¸­æ˜¯å¦åŒ…å«çŸ­è¯­æˆ–å•è¯ï¼ˆå®½æ¾åŒ¹é…ï¼‰
            ab_text = (a + " " + b).lower()
            phrase_words = phrase.lower().split()
            word_in_dialog = w in ab_text
            phrase_words_in_dialog = any(pw in ab_text for pw in phrase_words if len(pw) > 2)
            
            if not word_in_dialog and not phrase_words_in_dialog:
                bad.append((w, f"phrase-dialog-mismatch: '{phrase}' not found in dialog"))
                ok_all = False
                break
        
        if not ok_all:
            continue
    
    return bad

def check_coverage(expected_words, items):
    """è¦†ç›–ç‡æ£€æŸ¥"""
    exp = [x.lower() for x in expected_words]
    got = [(it.get("word") or "").lower() for it in items]
    missing = sorted(set(exp) - set(got))
    extras = sorted(set(got) - set(exp))
    dupes = sorted([w for w in got if got.count(w) > 1])
    return (len(missing) == 0 and len(extras) == 0), {"missing": missing, "extras": extras, "dupes": dupes}

def validate_json_schema(data):
    """JSON Schema éªŒè¯"""
    try:
        from jsonschema import validate
        validate(instance=data, schema=SCHEMA)
        return True, ""
    except Exception as e:
        return False, str(e)

def clean_passed_words_from_failed_permanent(passed_words):
    """ä» Resource/failed_permanent.json ä¸­æ¸…ç†å·²é€šè¿‡éªŒè¯çš„å•è¯"""
    failed_file = os.path.join("Resource", "failed_permanent.json")
    
    if not os.path.exists(failed_file):
        print("[INFO] failed_permanent.json ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
        return True
    
    try:
        # è¯»å–å¤±è´¥è¯åˆ—è¡¨
        with open(failed_file, "r", encoding="utf-8") as f:
            failed_words = json.load(f)
        
        if not isinstance(failed_words, list):
            print(f"[WARNING] {failed_file} æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡æ¸…ç†")
            return False
        
        print(f"[INFO] è¯»å–æ°¸ä¹…å¤±è´¥è¯åˆ—è¡¨ï¼š{len(failed_words)} ä¸ª")
        
        # è·å–é€šè¿‡éªŒè¯çš„å•è¯ï¼ˆå°å†™ï¼‰
        passed_word_set = {(word.get("word") or "").lower() for word in passed_words}
        print(f"[SUCCESS] é€šè¿‡éªŒè¯çš„å•è¯ï¼š{sorted(passed_word_set)}")
        
        # ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤é€šè¿‡éªŒè¯çš„å•è¯
        original_count = len(failed_words)
        cleaned_words = [word for word in failed_words if word.lower() not in passed_word_set]
        removed_count = original_count - len(cleaned_words)
        
        if removed_count > 0:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_dir = os.path.join("Resource", "backup")
            os.makedirs(backup_dir, exist_ok=True)
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = os.path.join(backup_dir, f"failed_permanent_before_clean_{timestamp}.json")
            try:
                import shutil
                shutil.copy2(failed_file, backup_file)
                print(f"[BACKUP] å¤‡ä»½å¤±è´¥è¯åˆ—è¡¨åˆ°: {backup_file}")
            except Exception as e:
                print(f"[WARNING] å¤‡ä»½å¤±è´¥ï¼š{e}")
            
            # å†™å…¥æ¸…ç†åçš„å¤±è´¥è¯åˆ—è¡¨
            with open(failed_file, "w", encoding="utf-8") as f:
                json.dump(cleaned_words, f, ensure_ascii=False, indent=2)
            
            print(f"[CLEAN] ä»æ°¸ä¹…å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤äº† {removed_count} ä¸ªå·²é€šè¿‡éªŒè¯çš„å•è¯")
            print(f"[INFO] å‰©ä½™æ°¸ä¹…å¤±è´¥å•è¯ï¼š{len(cleaned_words)} ä¸ª")
            
            if len(cleaned_words) == 0:
                print("[SUCCESS] æ‰€æœ‰æ°¸ä¹…å¤±è´¥çš„å•è¯éƒ½å·²é€šè¿‡éªŒè¯ï¼")
        else:
            print("[INFO] æ— éœ€æ¸…ç†ï¼Œæ²¡æœ‰æ‰¾åˆ°å·²é€šè¿‡éªŒè¯çš„å•è¯")
        
        return True
        
    except Exception as e:
        print(f"[ERROR] æ¸…ç†æ°¸ä¹…å¤±è´¥è¯åˆ—è¡¨å¤±è´¥ï¼š{e}")
        return False

def merge_to_words_json(new_words):
    """å°†éªŒè¯é€šè¿‡çš„è¯æ±‡åˆå¹¶åˆ° Resource/words.json"""
    words_file = os.path.join("Resource", "words.json")
    
    # è¯»å–ç°æœ‰è¯åº“
    existing_words = []
    if os.path.exists(words_file):
        try:
            with open(words_file, "r", encoding="utf-8") as f:
                existing_words = json.load(f)
            print(f"ğŸ“š è¯»å–ç°æœ‰è¯åº“ï¼š{len(existing_words)} ä¸ªè¯")
        except Exception as e:
            print(f"âŒ è¯»å– {words_file} å¤±è´¥ï¼š{e}")
            return False
    
    # åˆ›å»ºç°æœ‰è¯æ±‡çš„å°å†™æ˜ å°„ï¼Œé¿å…é‡å¤
    existing_word_map = {(word.get("word") or "").lower(): word for word in existing_words}
    
    # åˆå¹¶æ–°è¯æ±‡
    added_count = 0
    updated_count = 0
    
    for new_word in new_words:
        word_key = (new_word.get("word") or "").lower()
        if word_key in existing_word_map:
            # æ›´æ–°ç°æœ‰è¯æ±‡
            existing_word_map[word_key] = new_word
            updated_count += 1
            print(f"ğŸ”„ æ›´æ–°è¯æ±‡: {word_key}")
        else:
            # æ·»åŠ æ–°è¯æ±‡
            existing_word_map[word_key] = new_word
            added_count += 1
            print(f"âœ… æ·»åŠ è¯æ±‡: {word_key}")
    
    # é‡æ–°æ„å»ºè¯æ±‡åˆ—è¡¨ï¼ˆä¿æŒå­—æ¯é¡ºåºï¼‰
    merged_words = list(existing_word_map.values())
    merged_words.sort(key=lambda x: (x.get("word") or "").lower())
    
    # å¤‡ä»½åŸæ–‡ä»¶
    if os.path.exists(words_file):
        backup_dir = os.path.join("Resource", "backup")
        os.makedirs(backup_dir, exist_ok=True)
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = os.path.join(backup_dir, f"words_before_merge_{timestamp}.json")
        try:
            import shutil
            shutil.copy2(words_file, backup_file)
            print(f"ğŸ’¾ å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
        except Exception as e:
            print(f"âš ï¸ å¤‡ä»½å¤±è´¥ï¼š{e}")
    
    # å†™å…¥åˆå¹¶åçš„è¯åº“
    try:
        with open(words_file, "w", encoding="utf-8") as f:
            json.dump(merged_words, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜åˆå¹¶åçš„è¯åº“ï¼š{len(merged_words)} ä¸ªè¯")
        print(f"ğŸ“Š æœ¬æ¬¡æ“ä½œï¼šæ–°å¢ {added_count} ä¸ªï¼Œæ›´æ–° {updated_count} ä¸ª")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜ {words_file} å¤±è´¥ï¼š{e}")
        return False

def main():
    """ä¸»éªŒè¯æµç¨‹"""
    temp_file = "temp_words.json"
    
    print("ğŸ” ç‹¬ç«‹è¯æ±‡éªŒè¯è„šæœ¬å¯åŠ¨...")
    print(f"ğŸ“‹ åŠŸèƒ½è¯ç™½åå•ï¼š{len(FUNCTION_WORDS)} ä¸ª")
    
    # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(temp_file):
        print(f"âŒ æ‰¾ä¸åˆ° {temp_file} æ–‡ä»¶")
        print(f"ğŸ“ è¯·å°†AIç”Ÿæˆçš„JSONç»“æœç²˜è´´åˆ° {temp_file} ä¸­")
        return False
    
    # è¯»å–å¹¶è§£æJSON
    try:
        with open(temp_file, "r", encoding="utf-8") as f:
            content = f.read().strip()
        
        if not content:
            print(f"âŒ {temp_file} æ–‡ä»¶ä¸ºç©º")
            return False
        
        print(f"ğŸ“„ è¯»å– {temp_file}ï¼š{len(content)} å­—ç¬¦")
        
        # è§£æJSON
        data = json.loads(content)
        print(f"âœ… JSONè§£ææˆåŠŸï¼š{type(data).__name__}")
        
        if isinstance(data, list):
            print(f"ğŸ“Š åŒ…å« {len(data)} ä¸ªè¯æ¡")
        else:
            print("âŒ æ•°æ®ä¸æ˜¯æ•°ç»„æ ¼å¼")
            return False
            
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥ï¼š{e}")
        return False
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return False
    
    # æå–æœŸæœ›çš„å•è¯åˆ—è¡¨ï¼ˆä»failed_permanent.jsonï¼‰
    expected_words = []
    failed_file = os.path.join("Resource", "failed_permanent.json")
    if os.path.exists(failed_file):
        try:
            with open(failed_file, "r", encoding="utf-8") as f:
                expected_words = json.load(f)
            print(f"ğŸ“‹ æœŸæœ›å¤„ç†çš„è¯æ±‡ï¼š{expected_words}")
        except Exception as e:
            print(f"âš ï¸ è¯»å– {failed_file} å¤±è´¥ï¼š{e}")
    
    if not expected_words:
        print("âš ï¸ æ— æ³•è·å–æœŸæœ›è¯æ±‡åˆ—è¡¨ï¼Œå°†è·³è¿‡è¦†ç›–ç‡æ£€æŸ¥")
    
    # å¼€å§‹éªŒè¯
    print("\n" + "="*50)
    print("ğŸ” å¼€å§‹éªŒè¯...")
    
    # 1. JSON Schema éªŒè¯
    print("\nğŸ“‹ 1. JSON Schema éªŒè¯...")
    schema_ok, schema_error = validate_json_schema(data)
    if schema_ok:
        print("âœ… JSON Schema éªŒè¯é€šè¿‡")
    else:
        print(f"âŒ JSON Schema éªŒè¯å¤±è´¥ï¼š{schema_error}")
        return False
    
    # 2. è´¨é‡æ£€æŸ¥
    print("\nğŸ” 2. è´¨é‡æ£€æŸ¥...")
    bad_items = quality_checks(data)
    if not bad_items:
        print("âœ… è´¨é‡æ£€æŸ¥é€šè¿‡")
    else:
        print(f"âŒ è´¨é‡æ£€æŸ¥å‘ç° {len(bad_items)} ä¸ªé—®é¢˜ï¼š")
        for word, reason in bad_items[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {word}: {reason}")
        if len(bad_items) > 10:
            print(f"   ... è¿˜æœ‰ {len(bad_items) - 10} ä¸ªé—®é¢˜")
    
    # 3. è¦†ç›–ç‡æ£€æŸ¥
    if expected_words:
        print("\nğŸ“Š 3. è¦†ç›–ç‡æ£€æŸ¥...")
        coverage_ok, coverage_info = check_coverage(expected_words, data)
        if coverage_ok:
            print("âœ… è¦†ç›–ç‡æ£€æŸ¥é€šè¿‡")
        else:
            print("âŒ è¦†ç›–ç‡æ£€æŸ¥å‘ç°é—®é¢˜ï¼š")
            if coverage_info["missing"]:
                print(f"   - ç¼ºå¤±è¯æ±‡ï¼š{coverage_info['missing']}")
            if coverage_info["extras"]:
                print(f"   - é¢å¤–è¯æ±‡ï¼š{coverage_info['extras']}")
            if coverage_info["dupes"]:
                print(f"   - é‡å¤è¯æ±‡ï¼š{coverage_info['dupes']}")
    else:
        coverage_ok = True
        print("\nğŸ“Š 3. è¦†ç›–ç‡æ£€æŸ¥ï¼šè·³è¿‡ï¼ˆæ— æœŸæœ›è¯æ±‡åˆ—è¡¨ï¼‰")
    
    # 4. ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ 4. ç»Ÿè®¡ä¿¡æ¯...")
    total_words = len(data)
    function_word_count = 0
    content_word_count = 0
    
    for item in data:
        word = (item.get("word") or "").lower().strip()
        if word in FUNCTION_WORDS:
            function_word_count += 1
        else:
            content_word_count += 1
    
    print(f"   - æ€»è¯æ•°ï¼š{total_words}")
    print(f"   - åŠŸèƒ½è¯ï¼š{function_word_count}")
    print(f"   - å†…å®¹è¯ï¼š{content_word_count}")
    
    # 5. æœ€ç»ˆç»“æœ
    print("\n" + "="*50)
    overall_ok = schema_ok and len(bad_items) == 0 and coverage_ok
    
    if overall_ok:
        print("ğŸ‰ éªŒè¯é€šè¿‡ï¼æ‰€æœ‰æ£€æŸ¥éƒ½æˆåŠŸ")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦åˆå¹¶åˆ°è¯åº“
        while True:
            response = input("\nâ“ æ˜¯å¦å°†è¿™äº›è¯æ±‡åˆå¹¶åˆ° Resource/words.jsonï¼Ÿ(y/n): ").strip().lower()
            if response in ['y', 'yes', 'æ˜¯', 'æ˜¯çš„']:
                print("\nğŸ”„ å¼€å§‹åˆå¹¶åˆ°è¯åº“...")
                merge_success = merge_to_words_json(data)
                if merge_success:
                    print("âœ… åˆå¹¶å®Œæˆï¼è¯åº“å·²æ›´æ–°")
                    
                    # è‡ªåŠ¨æ¸…ç†å·²é€šè¿‡éªŒè¯çš„å•è¯ä»æ°¸ä¹…å¤±è´¥åˆ—è¡¨
                    print("\nğŸ§¹ è‡ªåŠ¨æ¸…ç†æ°¸ä¹…å¤±è´¥åˆ—è¡¨...")
                    clean_success = clean_passed_words_from_failed_permanent(data)
                    if clean_success:
                        print("âœ… æ°¸ä¹…å¤±è´¥åˆ—è¡¨æ¸…ç†å®Œæˆ")
                    else:
                        print("âš ï¸ æ°¸ä¹…å¤±è´¥åˆ—è¡¨æ¸…ç†å¤±è´¥")
                    
                    # æ¸…ç©ºä¸´æ—¶æ–‡ä»¶
                    try:
                        with open(temp_file, "w", encoding="utf-8") as f:
                            json.dump([], f, ensure_ascii=False, indent=2)
                        print(f"ğŸ—‘ï¸  å·²æ¸…ç©º {temp_file}")
                    except Exception as e:
                        print(f"âš ï¸ æ¸…ç©º {temp_file} å¤±è´¥ï¼š{e}")
                else:
                    print("âŒ åˆå¹¶å¤±è´¥")
                    return False
                break
            elif response in ['n', 'no', 'å¦', 'ä¸']:
                print("â¸ï¸  è·³è¿‡åˆå¹¶ï¼ŒéªŒè¯å®Œæˆ")
                break
            else:
                print("â“ è¯·è¾“å…¥ y/n")
        
        print("âœ… æ•°æ®éªŒè¯æˆåŠŸï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨")
    else:
        print("âŒ éªŒè¯å¤±è´¥ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š")
        if not schema_ok:
            print("   - JSON Schema ä¸ç¬¦åˆè¦æ±‚")
        if bad_items:
            print(f"   - {len(bad_items)} ä¸ªè¯æ±‡å­˜åœ¨è´¨é‡é—®é¢˜")
        if not coverage_ok:
            print("   - è¦†ç›–ç‡ä¸å®Œæ•´")
        print("ğŸ”§ è¯·ä¿®å¤é—®é¢˜åé‡æ–°éªŒè¯")
    
    return overall_ok

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)