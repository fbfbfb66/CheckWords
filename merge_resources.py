#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
merge_resources.py - Resourceæ•´åˆè„šæœ¬

åŠŸèƒ½ï¼š
1. å°†Resource2,3,4çš„words.jsonå†…å®¹æ•´åˆåˆ°Resource/words.jsonä¸­
2. å»é‡åˆå¹¶è¯æ±‡æ•°æ®
3. å°†Resource2,3,4é‡ç½®ä¸ºåˆå§‹çŠ¶æ€
4. ä¿ç•™å„è‡ªçš„API keyé…ç½®
"""

import os
import json
import shutil
from datetime import datetime

# åŸºæœ¬é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RESOURCE_DIRS = ["Resource2", "Resource3", "Resource4"]
TARGET_DIR = "Resource"

# GLM API Keys for each resource (ç»Ÿä¸€ä½¿ç”¨GLM-4.6)
API_KEYS = {
    "Resource2": "06b420079b514b599bbd6a6381829677.7j0V6fafUrjWg1sl",
    "Resource3": "06b420079b514b599bbd6a6381829677.7j0V6fafUrjWg1sl",
    "Resource4": "06b420079b514b599bbd6a6381829677.7j0V6fafUrjWg1sl"
}

# OFFSETé…ç½® (é‡ç½®åéƒ½ä¸ºåˆå§‹å€¼ï¼Œé¿å…é‡å¤è·å–)
OFFSETS = {
    "Resource2": 1000,     # é‡ç½®åä»1000å¼€å§‹
    "Resource3": 2000,     # é‡ç½®åä»2000å¼€å§‹
    "Resource4": 3000      # é‡ç½®åä»3000å¼€å§‹
}

def load_words_json(file_path):
    """åŠ è½½words.jsonæ–‡ä»¶"""
    if not os.path.exists(file_path):
        return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except Exception as e:
        print(f"âš ï¸ è¯»å– {file_path} å¤±è´¥: {e}")
        return []

def save_words_json(file_path, data):
    """ä¿å­˜words.jsonæ–‡ä»¶"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜ {file_path} å¤±è´¥: {e}")
        return False

def merge_word_data(existing_words, new_words):
    """åˆå¹¶è¯æ±‡æ•°æ®ï¼Œå»é‡"""
    # åˆ›å»ºå·²å­˜åœ¨è¯æ±‡çš„å­—å…¸ (å°å†™é”®å€¼)
    existing_dict = {}
    for word in existing_words:
        if isinstance(word, dict) and 'word' in word:
            key = word['word'].lower()
            existing_dict[key] = word
    
    # åˆå¹¶æ–°è¯æ±‡
    added_count = 0
    for word in new_words:
        if isinstance(word, dict) and 'word' in word:
            key = word['word'].lower()
            if key not in existing_dict:
                existing_dict[key] = word
                added_count += 1
    
    # è¿”å›åˆå¹¶åçš„åˆ—è¡¨
    merged_list = list(existing_dict.values())
    return merged_list, added_count

def reset_resource_to_initial(resource_dir):
    """å°†Resourceé‡ç½®ä¸ºåˆå§‹çŠ¶æ€"""
    resource_path = os.path.join(BASE_DIR, resource_dir)
    
    print(f"ğŸ”„ é‡ç½® {resource_dir} åˆ°åˆå§‹çŠ¶æ€...")
    
    # 1. æ¸…ç©ºå¹¶é‡å»ºbackupæ–‡ä»¶å¤¹
    backup_path = os.path.join(resource_path, "backup")
    if os.path.exists(backup_path):
        shutil.rmtree(backup_path)
    os.makedirs(backup_path)
    
    # 2. æ¸…ç©ºå¹¶é‡å»ºlogsæ–‡ä»¶å¤¹
    logs_path = os.path.join(resource_path, "logs")
    if os.path.exists(logs_path):
        shutil.rmtree(logs_path)
    os.makedirs(logs_path)
    
    # 3. é‡ç½®failed_permanent.jsonä¸ºç©ºæ•°ç»„
    failed_perm_path = os.path.join(resource_path, "failed_permanent.json")
    with open(failed_perm_path, 'w', encoding='utf-8') as f:
        json.dump([], f, ensure_ascii=False, indent=2)
    
    # 4. é‡ç½®function_words_custom.txtä¸ºç©º
    func_words_path = os.path.join(resource_path, "function_words_custom.txt")
    with open(func_words_path, 'w', encoding='utf-8') as f:
        f.write("")
    
    # 5. é‡ç½®words.jsonä¸ºç©ºæ•°ç»„
    words_path = os.path.join(resource_path, "words.json")
    with open(words_path, 'w', encoding='utf-8') as f:
        json.dump([], f, ensure_ascii=False, indent=2)
    
    # 6. é‡ç½®pipeline.pyçš„é…ç½®ä½†ä¿æŒAPI key
    reset_pipeline_config(resource_dir)
    
    # 7. é‡ç½®state.json
    state_path = os.path.join(resource_path, "state.json")
    initial_state = {
        "run_id": "off0-tot100-bs8",
        "done_batches": []
    }
    with open(state_path, 'w', encoding='utf-8') as f:
        json.dump(initial_state, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {resource_dir} å·²é‡ç½®ä¸ºåˆå§‹çŠ¶æ€")

def reset_pipeline_config(resource_dir):
    """é‡ç½®pipeline.pyé…ç½®ä½†ä¿æŒAPI key"""
    pipeline_path = os.path.join(BASE_DIR, resource_dir, "pipeline.py")
    
    # è¯»å–å½“å‰pipeline.py
    with open(pipeline_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–å¯¹åº”çš„API keyå’ŒOFFSET
    api_key = API_KEYS.get(resource_dir)
    offset = OFFSETS.get(resource_dir, 0)
    
    # é‡ç½®å…³é”®é…ç½®é¡¹
    modifications = [
        ('TOTAL_WORDS     = \\d+', 'TOTAL_WORDS     = 1000'),
        ('AUTO_OFFSET     = \\w+', 'AUTO_OFFSET     = False'),
        (f'OFFSET          = \\s*\\d+', f'OFFSET          = {offset}'),
        ('GLM_API_KEY     = "[^"]*"', f'GLM_API_KEY     = "{api_key}"'),
        ('MODEL_NAME      = "[^"]*"', 'MODEL_NAME      = "glm-4.6"')
    ]
    
    import re
    for pattern, replacement in modifications:
        content = re.sub(pattern, replacement, content)
    
    # å†™å›æ–‡ä»¶
    with open(pipeline_path, 'w', encoding='utf-8') as f:
        f.write(content)

def main():
    print("ğŸš€ å¼€å§‹Resourceæ•´åˆ...")
    print("=" * 60)
    
    # 1. åŠ è½½Resourceçš„ç°æœ‰è¯æ±‡
    resource_words_path = os.path.join(BASE_DIR, TARGET_DIR, "words.json")
    existing_words = load_words_json(resource_words_path)
    print(f"ğŸ“š Resourceç°æœ‰è¯æ±‡: {len(existing_words)} ä¸ª")
    
    # 2. æ”¶é›†æ‰€æœ‰Resource2,3,4çš„è¯æ±‡
    all_new_words = []
    for resource_dir in RESOURCE_DIRS:
        words_path = os.path.join(BASE_DIR, resource_dir, "words.json")
        words = load_words_json(words_path)
        all_new_words.extend(words)
        print(f"ğŸ“– {resource_dir}è¯æ±‡: {len(words)} ä¸ª")
    
    print(f"ğŸ“‹ å¾…æ•´åˆè¯æ±‡æ€»æ•°: {len(all_new_words)} ä¸ª")
    
    # 3. åˆå¹¶å»é‡
    print("ğŸ”„ å¼€å§‹åˆå¹¶å»é‡...")
    merged_words, added_count = merge_word_data(existing_words, all_new_words)
    print(f"âœ¨ åˆå¹¶å®Œæˆ: æ–°å¢ {added_count} ä¸ªè¯æ±‡")
    print(f"ğŸ“Š Resourceè¯åº“æ€»æ•°: {len(merged_words)} ä¸ª")
    
    # 4. å¤‡ä»½Resourceçš„åŸå§‹æ•°æ®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(BASE_DIR, TARGET_DIR, "backup", f"words_before_merge_{timestamp}.json")
    os.makedirs(os.path.dirname(backup_path), exist_ok=True)
    save_words_json(backup_path, existing_words)
    print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²å¤‡ä»½åˆ°: {os.path.basename(backup_path)}")
    
    # 5. ä¿å­˜åˆå¹¶åçš„æ•°æ®åˆ°Resource
    if save_words_json(resource_words_path, merged_words):
        print(f"âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜åˆ° {TARGET_DIR}/words.json")
    else:
        print("âŒ ä¿å­˜åˆå¹¶æ•°æ®å¤±è´¥ï¼Œåœæ­¢åç»­æ“ä½œ")
        return False
    
    # 6. é‡ç½®Resource2,3,4ä¸ºåˆå§‹çŠ¶æ€
    print("\n" + "=" * 60)
    print("ğŸ”„ å¼€å§‹é‡ç½®Resource2,3,4...")
    
    for resource_dir in RESOURCE_DIRS:
        reset_resource_to_initial(resource_dir)
    
    # 7. ç”Ÿæˆæ•´åˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•´åˆå®ŒæˆæŠ¥å‘Š:")
    print(f"  â€¢ ResourceåŸæœ‰è¯æ±‡: {len(existing_words)} ä¸ª")
    print(f"  â€¢ å¾…æ•´åˆè¯æ±‡æ€»æ•°: {len(all_new_words)} ä¸ª") 
    print(f"  â€¢ å®é™…æ–°å¢è¯æ±‡: {added_count} ä¸ª")
    print(f"  â€¢ Resourceè¯åº“æ€»æ•°: {len(merged_words)} ä¸ª")
    print(f"  â€¢ Resource2,3,4 å·²é‡ç½®ä¸ºåˆå§‹çŠ¶æ€ (OFFSET=0)")
    print(f"  â€¢ å„è‡ªAPI keyå·²ä¿ç•™")
    print("=" * 60)
    print("ğŸ‰ æ•´åˆå®Œæˆï¼")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("\nâœ… æ‰€æœ‰æ“ä½œæˆåŠŸå®Œæˆï¼")
        else:
            print("\nâŒ æ“ä½œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼")
    except Exception as e:
        print(f"\nğŸ’¥ è„šæœ¬æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()